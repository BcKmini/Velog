<h1 id="🎯-목차">🎯 목차</h1>
<blockquote>
<p>3.1 퍼셉트론에서 신경망으로 
__3.1.1 신경망의 예 
__3.1.2 퍼셉트론 복습 
__3.1.3 활성화 함수의 등장 
3.2 활성화 함수 
__3.2.1 시그모이드 함수 
__3.2.2 계단 함수 구현하기 
__3.2.3 계단 함수의 그래프 
__3.2.4 시그모이드 함수 구현하기 
__3.2.5 시그모이드 함수와 계단 함수 비교 
__3.2.6 비선형 함수 
__3.2.7 ReLU 함수 
3.3 다차원 배열의 계산 
__3.3.1 다차원 배열 
__3.3.2 행렬의 내적 
__3.3.3 신경망의 내적 
3.4 3층 신경망 구현하기 
__3.4.1 표기법 설명 
__3.4.2 각 층의 신호 전달 구현하기 
__3.4.3 구현 정리 
3.5 출력층 설계하기 
__3.5.1 항등 함수와 소프트맥스 함수 구현하기 
__3.5.2 소프트맥스 함수 구현 시 주의점 
__3.5.3 소프트맥스 함수의 특징 
__3.5.4 출력층의 뉴런 수 정하기
3.6 손글씨 숫자 인식 
__3.6.1 MNIST 데이터셋 
__3.6.2 신경망의 추론 처리 
__3.6.3 배치 처리 </p>
</blockquote>
<hr />
<h1 id="📌-31-퍼셉트론에서-신경망으로">📌 3.1 퍼셉트론에서 신경망으로</h1>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/1611bceb-135a-48fb-80a1-0bd6f0895e5d/image.png" /></p>
<p>입력층
출력층
은닉층</p>
<blockquote>
<p>신경망은 모두 3층으로 구성되지만, 가중치를 갖는 층은 2개뿐이기 때문에 '2층 신경망'이라고도 한다. </p>
</blockquote>
<h2 id="📌-312-퍼셉트론-복습">📌 3.1.2 퍼셉트론 복습</h2>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/3ae8bf43-ed94-4a66-99d1-c839c1def816/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/61604a03-f8ce-436f-a714-01a3f1c54683/image.png" /></th>
</tr>
</thead>
<tbody><tr>
<td>- x1과 x2라는 두 신호를 입력받아 y를 출력한다.</td>
<td></td>
</tr>
<tr>
<td>- b는 편향을 나타내는 매개변수로, 뉴런이 얼마나 쉽게 활성화되느냐를 제어한다.</td>
<td></td>
</tr>
<tr>
<td>(w1, w2는 각 신호의 가중치를 나타내는매개변수로, 각 신호의 영향력을 제어한다.)</td>
<td></td>
</tr>
</tbody></table>
<h2 id="📌-313-활성화-함수의-등장">📌 3.1.3 활성화 함수의 등장</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/93f6001e-3bde-4385-9295-9aa50bd133fb/image.png" />
입력 신호의 총합을 출력 신호로 변환하는 함수를 <span style="color: indianred;">활성화 함수</span>라고 한다. -&gt; 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할을 한다. </p>
<blockquote>
<p><span style="color: indianred;">단순 퍼셉트론</span>은 단층 네트워크에서 계단 함수(임계값을 경계로 출력이 바뀌는 함수)를 활성화 함수로 사용한 모델을 가르키고 <span style="color: indianred;">
다층 퍼세트론</span>은 신경망(여러 층으로 구성되고 시그모이드 함수의 매끈한 활성화 함수를 사용하는 네트워크)를 가르킴 </p>
</blockquote>
<hr />
<h1 id="📌-32-활성화-함수">📌 3.2 활성화 함수</h1>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/64603045-0f7d-4eda-ac1a-834cff6df9e2/image.png" /></p>
<p>-&gt; 활성화 함수는 임계값을 경게로 출력이 바뀌는데, 이런 함수를 <span style="color: indianred;">계단 함수</span>라고 한다. (퍼셉트론에서 자주 쓰임)</p>
<h2 id="📌-321-시그모이드-함수">📌 3.2.1 시그모이드 함수</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/15a762b7-adb8-451c-8cf6-487b9b8499d0/image.png" /></p>
<p>신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하고, 그 변환된 신호를 다음 뉴런에 전달한다. </p>
<h2 id="📌-322-계단-함수-구현하기">📌 3.2.2 계단 함수 구현하기</h2>
<pre><code class="language-py"># 입력이 0을 넘으면 1을 출력하고 그 외에는 0을 출력
def step_function(x):
    if x &gt; 0:
        return 1
    else:
        return 0</code></pre>
<pre><code class="language-py"># 넘파일 배열 지원하도록 수정
def step_function(x):
    y = x &gt; 0
    return y.astype(np.int)</code></pre>
<pre><code class="language-py">import numpy as np
x = np.array([-1, 1.0, 2.0)]
x

y = x &gt; 0 
y

# 결과는 bool 배열이 생성</code></pre>
<pre><code class="language-py"># int형으로 변환
y = yastype(np.int)
y</code></pre>
<h2 id="📌-323-계단-함수의-그래프">📌 3.2.3 계단 함수의 그래프</h2>
<pre><code class="language-py">import numpy as np
import matplotlib.pylab as plt

def step_function(x):
    return np.array(x &gt; 0, dtype=int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)  # y축의 범위 지정
plt.show()</code></pre>
<h2 id="📌-324-시그모이드-함수-구현하기">📌 3.2.4 시그모이드 함수 구현하기</h2>
<pre><code class="language-py">def sigmoid(x):
    return 1 / (1 + np.exp(-x))</code></pre>
<pre><code class="language-py"># 실제로 넘파이 배열을 제대로 처리 했는지 확인
x = np.array([-1.0, 1.0, 2.0])
sigmoid(x)
# array([ 0.26894142, 0.73105858, 0.88079708])</code></pre>
<pre><code class="language-py"># 넘파이 -&gt; 브로드캐스트 기능
t = np.array([1.0, 2.0, 3.0])
1.0 + t
# array([2., 3., 4.])
1.0 / t
</code></pre>
<pre><code class="language-py"># 시그모이므 함수 그래프
import numpy as np
import matplotlib.pylab as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.arange(-5.0, 5.0, 0.1)  # np.array -&gt; np.arange
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)  # y축 범위 지정
plt.show()
</code></pre>
<h2 id="📌-325-시그모이드-함수와-계단-함수-비교">📌 3.2.5 시그모이드 함수와 계단 함수 비교</h2>
<blockquote>
<h4 id="공통점">공통점</h4>
<p>비선형 함수: 둘 다 입력값을 비선형적으로 변환하여 복잡한 패턴을 학습할 수 있게 해줍니다.
이진 결정 가능: 특정 임계값에 따라 입력을 활성화(1) 또는 비활성화(0)로 결정할 수 있습니다.
-&gt; 입력이 중요하면 큰 값을 출력하고 입력이 중요하지 않으면 작은 값을 출력함</p>
</blockquote>
<blockquote>
<h4 id="차이점">차이점</h4>
<p>출력 범위: 계단 함수는 0 또는 1의 값을 반환하지만, 시그모이드 함수는 0과 1 사이의 연속적인 값을 반환합니다. 시그모이드는 실수 범위로 연산이 가능하여 경사하강법에서 미분이 가능합니다.
미분 가능 여부: 시그모이드 함수는 미분이 가능하여 역전파 과정에서 유리하지만, 계단 함수는 미분이 불가능해 학습 과정에서 사용하기 어렵습니다.</p>
</blockquote>
<h2 id="📌-326-비선형-함수">📌 3.2.6 비선형 함수</h2>
<p>신경망에서 활성화 함수로 비선형 함수를 사용하는 이유는 층을 쌓는 것의 이점을 최대한 활용하기 위해서이다. 만약 활성화 함수로 선형 함수를 사용한다면, 신경망의 모든 층을 통과해도 여전히 전체 네트워크가 하나의 선형 변환으로 축약됩니다. 이는 결국 네트워크가 단일 층으로 구성된 경우와 동일하게 되어, 복잡한 패턴을 학습하는 데 필요한 다층 구조의 표현력을 상실하게 된다. </p>
<blockquote>
<p>선형 함수를 사용하면 각 층에서 가중치와 입력의 선형 조합만을 수행하므로, 여러 층을 추가하더라도 여전히 단일한 선형 변환이 됩니다. 예를 들어, 두 개의 선형 변환 f(x)=a⋅xf(x)=a⋅x와 g(x)=b⋅xg(x)=b⋅x를 결합하면 결과는 단순히 h(x)=c⋅xh(x)=c⋅x로 나타낼 수 있습니다. 이는 두 층이 단일 선형 변환으로 표현될 수 있음을 의미합니다. 따라서 네트워크가 아무리 깊어도 결과적으로 하나의 선형 변환과 다를 바 없게 됩니다.</p>
</blockquote>
<p>따라서, 층을 쌓는 혜택을 얻고, 신경망이 다양한 패턴을 학습할 수 있게 하려면 반드시 비선형 활성화 함수를 사용해야 합니다</p>
<h2 id="📌-327-relu-함수">📌 3.2.7 ReLU 함수</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/daf40a69-c6f2-4cec-b1f8-6927cbd51fef/image.png" /></p>
<p>입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수이다.</p>
<pre><code class="language-py"># ReLU 구현
def relu(x):
    return np.maximum(0, x)
# maximum 은 두 입력 중 큰 값을 선택해 반환하는 함수</code></pre>
<pre><code class="language-py"># Relu 그래프 그리기 

import numpy as np
import matplotlib.pylab as plt

def relu(x):
    return np.maximum(0, x)

# x 값 생성
x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)

# 그래프 그리기
plt.plot(x, y)
plt.title("ReLU Function")
plt.xlabel("x")
plt.ylabel("ReLU(x)")
plt.ylim(-1, 5.5)  # y축 범위 지정
plt.grid()
plt.show()
</code></pre>
<hr />
<h1 id="📌-33-다차원-배열의-계산">📌 3.3 다차원 배열의 계산</h1>
<p>다차월 배열을 사용한 계산법을 숙달하면 신경망을 효율적으로 구현 가능</p>
<h2 id="📌-331-다차원-배열">📌 3.3.1 다차원 배열</h2>
<pre><code class="language-py"># 1차원 배열
import numpy as np
A = np.array([1,2,3,4])
print(A)

np.ndim(A) # 배열의 차원수 
A.shape
</code></pre>
<p>2차원 배열은 행렬이라고 한다.
가로 -&gt; 행
세로 -&gt; 열</p>
<pre><code class="language-py"># 2차원 배열
B = np.array([[1,2], [3,4], [5,6]])
print(B)

np.ndim(B)

B.shape</code></pre>
<h1 id="📌-332-행렬의-곱">📌 3.3.2 행렬의 곱</h1>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/a7de7c35-76be-4357-9d99-1d5391fde50a/image.png" /></p>
<pre><code class="language-py">A = np.array([[1,2], [3,4]])
A.shape
# (2,2)
B = np.array([[5,6], [7,8]])
B.shape
# (2,2)
np.dot(A,B) # 두 행렬의 곱 
# 결과값 -&gt; array([[19,22],
     # [43, 50]])</code></pre>
<h3 id="피연산자의-순서가-다르면-결과도-다르다">피연산자의 순서가 다르면 결과도 다르다.</h3>
<pre><code class="language-py"># 2X3 행렬과 3X2 행렬의 곱을 구현

A = np.array([[1,2,3], [4,5,6]])
A.shape
# (2,3)
B = np.array([[1,2], [3,4], [5,6]])
B.shape

np.dot(A,B)
</code></pre>
<h3 id="행렬--a의-1번째-차원의-원소-수열-수와-행렬-b의-0번째-차원의-원소-수행-수가-같아야-한다">행렬  A의 1번째 차원의 원소 수(열 수)와 행렬 B의 0번째 차원의 원소 수(행 수)가 같아야 한다.</h3>
<blockquote>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/7ebdc938-214c-4475-99f0-84bce954ad29/image.png" /></p>
</blockquote>
<pre><code class="language-py">import numpy as np

A = np.array([[1, 2, 3], [4, 5, 6]])
C = np.array([[1, 2], [3, 4], [5, 6]])  

result = np.dot(A, C)
print(result)
</code></pre>
<pre><code class="language-py"># 예시 2
A = np.array([[1,2], [3,4], [4,6]])
A.shape

B = np.array([7,8])
B.shape

np.dot(A, B)
</code></pre>
<h2 id="📌-333-신경망에서의-행렬-곱">📌 3.3.3 신경망에서의 행렬 곱</h2>
<p>넘파이 행렬을 써서 신경망을 구축해보기 
-&gt; 이 신경망은 편향과 활성화 함수를 생략하고 가중치만 갖는다. 
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/0517694b-1adb-4423-922e-f32535c20f83/image.png" /></p>
<blockquote>
<p>X, W, Y의 형상을 주의해서 보기
X와 W의 대응하는 차원의 원소 수가 같아야 한다는 걸 명시</p>
</blockquote>
<pre><code class="language-py">X = np.array([1,2])
X.shape

W = np.array([[1,3,5], [2,4,6]])
print(W)

W.shape

Y = np.dot(X, W)
print(Y)
</code></pre>
<hr />
<h1 id="📌-34-3층-신경망-구현하기">📌 3.4 3층 신경망 구현하기</h1>
<blockquote>
<p>넘파이의 다차원 배열을 사용해서 구현(입력신호, 가중치, 편향은 적당한 값으로 설정)
W1은 2X3 행렬, X는 원소가 2개인 1차원 배열
-&gt; W1과 X의 대응하는 차원의 원소수가 같다. </p>
</blockquote>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/3aa5065c-6a7c-4477-b3f2-4122c7483d97/image.png" /></p>
<pre><code class="language-py">X = np.array([1.0, 0.5])
W1 = np.array([0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

print(W1.shape) # (2, 3)
print(X.shape) # (2,)
print(B1.shape) # (3,)

A = np.dot(X, W1) + B1</code></pre>
<blockquote>
<p>활성화 함수로 시그모이드 함수를 사용하기로 한다.</p>
</blockquote>
<pre><code class="language-py">Z1 = sigmoid(A1)

print(A1)
print(Z1)</code></pre>
<blockquote>
<p>1층에서 2층으로의 신호 전달 
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/dc55454e-ab74-4ece-a340-2c75eedc9e17/image.jpg" /></p>
</blockquote>
<pre><code class="language-py">W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
B2 = np.array([0.1, 0.2])

print(Z1.shape)
print(W2.shape)
print(B2.shape)

A2 = np.dot(Z1, W2) + B2
Z2 = simoid(A2)</code></pre>
<blockquote>
<p>2층에서 출력층으로서의 전달
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/74a227f5-47d2-4af6-acfa-31cefaff1bd7/image.png" /></p>
</blockquote>
<pre><code class="language-py">def identity_function(x):
    return x
W3 = np.array([[0.1, 0.3], [0.2, 0.4]]
B3 = np.array([0.1, 0.2])

A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3)</code></pre>
<blockquote>
<p>출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 정한다. 
예를 들어 회귀에는 항등함수를
2클래스 분류에는 시그모이드 함수를
다중 클래스 분류에는 소프트맥스 함수를 
사용하는 것이 일반적이다.</p>
</blockquote>
<h1 id="📌-343-구현-정리">📌 3.4.3 구현 정리</h1>
<pre><code class="language-py">import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def identity_function(x):
    return x

def init_network():
    network = {}
    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0.3])
    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
    network['b2'] = np.array([0.1, 0.2])
    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
    network['b3'] = np.array([0.1, 0.2])

    return network

def forward(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']

    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = identity_function(a3)

    return y

network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)

print(y)</code></pre>
<hr />
<h1 id="📌-35-출력층-설계하기">📌 3.5 출력층 설계하기</h1>
<p>일반적으로 회귀에는 항등 함수를, 분류에는 소트프맥스 함수를 사용한다. -&gt; 기계학습 문제는 분류화 회귀로 나뉜다.</p>
<blockquote>
<ol>
<li>분류 (Classification)
 정의: 분류 문제는 데이터를 사전 정의된 범주 또는 클래스에 할당하는 문제입니다.
 예시: 스팸 이메일 분류, 손글씨 숫자 인식, 이미지에서 고양이와 개 구분 등.
 출력: 클래스 레이블, 예를 들어 0 또는 1(이진 분류), 혹은 다수의 범주 중 하나.
 기법: 의사결정 트리, 로지스틱 회귀, k-최근접 이웃(KNN), 서포트 벡터 머신(SVM), 인공 신경망 등.</li>
</ol>
</blockquote>
<blockquote>
<ol start="2">
<li>회귀 (Regression)
 정의: 회귀 문제는 연속적인 숫자 예측 값을 출력하는 문제입니다.
 예시: 주택 가격 예측, 온도 예측, 주식 시장 가격 예측 등.
 출력: 실수 값(예: 주택 가격 $320,000).
 기법: 선형 회귀, 라쏘 회귀, 릿지 회귀, 의사결정 트리, 랜덤 포레스트, 인공 신경망 등.</li>
</ol>
</blockquote>
<h2 id="📌-351-항등-함수와-소프트맥스-함수-구현하기">📌 3.5.1 항등 함수와 소프트맥스 함수 구현하기</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/06722d2d-bae7-4b00-a1a8-02ed138af5ee/image.png" /></p>
<p>항등 함수는 입력을 그대로 출력한다. 입력과 출력이 항상 같다는 뜻의 항등이다. -&gt; 출력층에서 항등 함수를 사용하면 입력 신호가 그대로 출력 신호가 된다. </p>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/8cbf7457-6a90-4e1f-897e-17c098d3a472/image.png" /></p>
<p>(분류에서 사용하는)소프트맥스 함수
출력은 모든 입력 신호로부터 화살표를 받는다. 출력층의 각 뉴런이 모든 입력 신호에서 영향을 받기 때문이다.</p>
<pre><code class="language-py">a = np.array([0.3, 2.9, 4.0])

exp_a = np.exp(a)
print(exp_a)

sum_exp_a = np.sum(exp_a)
print(sum_exp_a)

y = exp_a / sum_exp_a
print(y)</code></pre>
<pre><code class="language-py"># 논리 흐름을 파이썬 함수로 정의

def softmax(a):
    exp_a = np.exp(a)
       sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a

    return y</code></pre>
<h2 id="📌-352-소프트맥스-함수-구현-시-주의점">📌 3.5.2 소프트맥스 함수 구현 시 주의점</h2>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/5489f6de-1ab4-4581-ba1b-0f877d9ccf54/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/a168ff48-89e1-4c7b-9bce-75f08b0e0767/image.png" /></th>
</tr>
</thead>
</table>
<p>앞 절에서 구현한 softmax() 함수는 컴퓨터로 게산할 때는 결함이 있다. (오버플로 문제) -&gt; 소프트맥스 함수는 지수 함수를 사용하는데, 지수 하수가 아주 큰 값을 내뱉기 때문이다.
큰 값끼리 나눗셈을 하면 결과 수치가 '불안정'해진다.</p>
<p>-&gt; 오버플로를 막을 목적으로는 입력 신호 중 최댓값을 이용하는 것이 일반적이다. </p>
<pre><code class="language-py">def softmax(a):
    c = np.max(a) # overflow 방지를위한 변수
    exp_a = np.exp(a - c) # overflow 대책 
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a

    return y</code></pre>
<h2 id="📌-353-소프트맥스-함수의-특징">📌 3.5.3 소프트맥스 함수의 특징</h2>
<blockquote>
<p>softmax() 함수를 사용하면 신경망의 출력은 다음과 같이 계산할 수 있다. </p>
</blockquote>
<pre><code class="language-py">a = np.array([0.3, 2.9, 4.0])
y = softmax(a)
print(y)

print(np.sum(y))</code></pre>
<blockquote>
<p>기계학습의 문제 풀이는 학습과 추론의 두 단계를 거쳐 이뤄진다. 학습 단게에서 모델링 학습하고, 추론 단게에서 앞서 학습한 모델로 미지의 데이터에 대해서 추론(분류)을 수행한다. 방금 설명한 대로, 추론 단게에서는 소프트맥스 함수를 생략하는 것이 일반적이다. 학습시킬 때는 출력층에서 소프트맥스 함수를 사용한다. </p>
</blockquote>
<hr />
<h1 id="📌-36-손글씨-숫자-인식">📌 3.6 손글씨 숫자 인식</h1>
<p>매개변수를 사용하여 학습 과정을 생략하고, 추론 과정만 구현할 것이다. 이 추론 과정을 순전파라고 한다. </p>
<h2 id="📌-361-mnist-데이터셋">📌 3.6.1 Mnist 데이터셋</h2>
<pre><code class="language-py">import sys, os
sys.path.append(os.pardir)  # 부모 디렉토리의 파일을 가져올 수 있도록 설정
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)

# 각 데이터의 형상 출력
print(x_train.shape)
print(t_train.shape)
print(x_test.shape)
print(t_test.shape)</code></pre>
<pre><code class="language-py">import sys, os
sys.path.append(os.pardir)  # 부모 디렉토리의 파일을 가져올 수 있도록 설정
import numpy as np
from dataset.mnist import load_mnist
from PIL import Image

def img_show(img):
    pil_img = Image.fromarray(np.uint8(img))
    pil_img.show()

(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)
# flatten=True로 설정해 읽어 들인 이미지는 1차원 넘파이 배열로 저장됩니다.

img = x_train[0]
label = t_train[0]
print(label)  # 5

print(img.shape)  # (784,)
img = img.reshape(28, 28)  # 2차원 형태로 변환
print(img.shape)  # (28, 28)

img_show(img)
</code></pre>
<h2 id="📌-362-신경망의-추론-처리">📌 3.6.2 신경망의 추론 처리</h2>
<pre><code class="language-py">#get_data() 함수
def get_data():    
    (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False, one_hot_label=False)
    return x_test, t_test

#init_network() 함수
def init_network():
    with open("sample_weight.pkl",'rb') as f:
        network = pickle.load(f)

    return network

#predict() 함수
def predict(network, x):
    W1,W2,W3 = network['W1'],network['W2'],network['W3']
    b1,b2,b3 = network['b1'],network['b2'],network['b3']

    a1 = np.dot(x,W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1,W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2,W3) + b3
    y = softmax(a3)

    return y</code></pre>
<blockquote>
<p>세 함수를 사용해 신경망에 의한 추론을 수행해보고, 정확도도 평가 해본다.</p>
</blockquote>
<pre><code class="language-py">x, t = get_data()
network = init_network()

accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i])
    p = np.argmax(y)
    if p == t[i]:
        accuracy_cnt += 1

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))

</code></pre>
<h2 id="📌-363-배치-처리">📌 3.6.3 배치 처리</h2>
<pre><code class="language-py">#x에 x_test(이미지)를 저장합니다.
x, _ = get_data()

#network 변수 딕셔너리 안에 저장된 매개변수들을 불러옵니다.
network = init_network()

W1, W2, W3 = network['W1'], network['W2'], network['W3']

#만장의 이미지가 2차원 배열로 잘 저장되어있음.
print(x.shape)
&gt;&gt; (10000, 784)

#출력층 노드 수가 784개, 첫번째 은닉층의 노드수가 50개 이므로
print(W1.shape)
&gt;&gt; (784, 50)

#첫번째 은닉층의 노드수가 50개, 두번째 은닉층의 노드수가 100개 이므로
print(W2.shape)
&gt;&gt; (50, 100)

#두번째 은닉층의 노드수가 100개, 출력층의 노드수가 10개 이므로
print(W3.shape)
&gt;&gt; (100, 10)</code></pre>
<pre><code class="language-py">x, t = get_data()
network = init_network()

batch_size = 100 # 배치 크기
accuracy_cnt = 0

#x_test의 길이(시험 이미지의 개수)만큼, 배치 크기 간격만큼 반복문을 수행
for i in range(0, len(x), batch_size):
    #batch_in에 입력 이미지를 배치 크기만큼 묶어서 보관해둠
    batch_in = x[i:i+batch_size]

    #batch_in의 저장된 한 묶음씩 predict()를 통과후 y에 저장
    y = predict(network, batch_in)

    #axis=1(각 행에 모든 열에서 동작)을 기준으로 최댓값의 인덱스 반환
    p = np.argmax(y, axis=1)

    #한 묶음씩 예측 성공한 개수를 다 더한후 총 accuracy_cnt에 저장
    accuracy_cnt += np.sum(p==t[i:i+batch_size])

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))</code></pre>
<pre><code class="language-py">list( range(0, 10) )
list( range(0,10,3) )</code></pre>
<h1 id="🚀-요약">🚀 요약</h1>
<blockquote>
<ul>
<li>신경망에서는 활성화 함수로 시그모이드 함수와 ReLU 함수 같은 매끄럽게 변화하는 함수를 이용한다.</li>
</ul>
</blockquote>
<ul>
<li>넘파이의 다차원 배열을 잘 사용하면 신경망을 효율적으로 구현할 수 있다.<ul>
<li>기계학습 문제는 크게 회귀와 분류로 나눌 수 있다.</li>
<li>출력층의 활성화 함수로는 회귀에서는 주로 항등 함수를, 분류에서는 주로 소프트맥스 함수를 이용한다.</li>
<li>분류에서는 출력층의 뉴런 수를 분류하려는 클래스 수와 같게 설정한다.</li>
</ul>
</li>
<li>입력 데이터를 묶은 것을 배치라 하며, 추론 처리를 이 배치 단위로 진행하면 결과를 훨씬 빠르게 얻을 수 있다.</li>
</ul>