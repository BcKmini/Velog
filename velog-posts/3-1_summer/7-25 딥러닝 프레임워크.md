<h3 id="딥러닝--프레임워크--deeplearning-framework">딥러닝  프레임워크  DeepLearning Framework</h3>
<h3 id="프레임워크framework">프레임워크(framework)</h3>
<p>⁻   응용 프로그램을 개발하기 위한 여러 라이브러리나 모듈 등을 효율적으로 사용할 수 있도록 하나로 
묶어 놓은 일종의 패키지
• 딥러닝 프레임워크 종류
⁻   TensorFlow, Keras, Theano, PyTorch, CNTK, Caffe, Mxnet, DL4J  등</p>
<h3 id="📌tensorflow-google">📌TensorFlow (Google)</h3>
<p>장점 : 
• 모델 세부 튜닝이 가능하여 디테일한 모델 설정 가능 
• 거의 모든 딥러닝 프로젝트 에 범용적으로 활용 가능
• 텐서보드를 통한 시각화
단점 : 
• 저수준 모델링 
= 높은 난이도</p>
<h3 id="📌pytorch-meta">📌PyTorch (Meta)</h3>
<p>장점 : 
• 익히기 쉽고 간결, 구현 빠름
• 비교적 빠른 최적화 가능
• 학습속도 빠름
• Visdom을 통한 시각화
단점 : 
• 텐서플로우에 비해 디테일한 
모델링 불가능
• 아직은 텐서플로우에 비해 부 
족한 사용자 수</p>
<h3 id="📌keras---tensorflow에-합쳐짐">📌Keras -&gt; TensorFlow에 합쳐짐</h3>
<p>장점 : 
• 사용자 친화성, 모듈성, 확장성
• 일관되고 간결한 API 제공
• 배우기 쉽고 모델 구축이 쉬움
단점 : 
• 함수 대부분 자동화 및 간편화 
되어 딥러닝 구조 파악 어려움
• 오류가 발생했을 때 케라스 자 
체의 문제인지, 백엔드 언어의 
문제인지 특정하기 어려움</p>
<h3 id="📌텐서플로우--tensorflow">📌텐서플로우  TensorFlow</h3>
<p>• 텐서플로우(TensorFlow)
⁻   구글의 딥러닝 프레임워크, 내부적으로 C/C++로 구현
⁻   파이썬 등 여러 가지 언어에서 접근할 수 있는 인터페이스 제공 
⁻   텐서(tensor)는 물리학에서 다차원 배열을 나타내는 용어
• 스칼라, 벡터, 행렬, 텐서 모두 지원 
⁻   플로우(flow)는 흐름</p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/043e7acb-edcf-4a9c-8080-567b460f239b/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/9af47f52-7004-4cca-88a3-b8b36632b189/image.png" /></th>
</tr>
</thead>
<tbody><tr>
<td>### Keras</td>
<td></td>
</tr>
<tr>
<td>• Keras</td>
<td></td>
</tr>
<tr>
<td>⁻   Python으로 작성</td>
<td></td>
</tr>
<tr>
<td>⁻   TensorFlow, CNTK, Theano에서 실행할 수 있는</td>
<td></td>
</tr>
<tr>
<td>고수준 딥러닝 API</td>
<td></td>
</tr>
<tr>
<td>⁻   쉽고 빠른 프로토타이핑이 가능</td>
<td></td>
</tr>
<tr>
<td>⁻   순방향 신경망, 컨볼루션 신경망과 반복적인 신경망 외 여러 가지의 조합도 지원</td>
<td></td>
</tr>
<tr>
<td>⁻   CPU 및 GPU에서 원활하게 실행</td>
<td></td>
</tr>
<tr>
<td>⁻   Tensorflow 2.0 버전 이상에는 Keras가 포함되어 있음</td>
<td></td>
</tr>
<tr>
<td>⁻   Keras는 신경망을 레고 조립하듯이 만들 수 있음</td>
<td></td>
</tr>
</tbody></table>
<p>• 설치 방법:
⁻   pip install tensorflow      (CPU 버전)
⁻   pip install tensorflow-gpu  (GPU 버전, 이는 CUDA 등 추가로 설치해야 함)</p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/02ca1208-b25a-477b-989d-4348dd2cb77f/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/c785acb3-092b-40a6-a358-edc67f5927e2/image.png" /></th>
</tr>
</thead>
<tbody><tr>
<td>&gt; ### Keras를  이용한  다층  퍼셉트론  구현</td>
<td></td>
</tr>
</tbody></table>
<pre><code class="language-py">from tensorflow.keras.models import Sequential # -&gt; Sequential -&gt; 순차적인
from tensorflow.keras.layers import Dense

model = Sequential() 

#모델 생성
model.add(Dense(units=64, activation='sigmoid', input_dim=100)) 
model.add(Dense(units=10, activation='sigmoid')) #Dense -&gt; 완전연결 구조

model.compile(loss='mse', optimizer='sgd', metrics=['accuracy'])

model.fit(X, y, epochs=5, batch_size=32)

loss_and_metrics = model.evaluate(X, y, batch_size=128)
classes = model.predict(new_X, batch_size=128)</code></pre>
<blockquote>
<h3 id="실습-mlp를-이용한--mnist-숫자인식">[실습] MLP를 이용한  MNIST 숫자인식</h3>
</blockquote>
<pre><code class="language-py">import tensorflow as tf

batch_size = 128    #가중치를 변경하기 전에 처리하는 샘플의 개수 
num_classes = 10    #출력 클래스의 개수
epochs = 50

#데이터를 학습 데이터와 테스트 데이터로 나눈다.
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() 

#입력 이미지를 2차원에서 1차원 벡터로 변경한다.
x_train = x_train.reshape(60000, 784) 
x_test = x_test.reshape(10000, 784)

#입력 이미지의 픽셀 값이 0.0에서 1.0 사이의 값이 되게 한다. 
x_train = x_train.astype('float32')
x_test = x_test.astype('float32') 
x_train /= 255
x_test /= 255

#클래스의 개수에 따라서 하나의 출력 픽셀만이 1이 되게 한다. 
#예를 들면 1 0 0 0 0 0 0 0 0 0과 같다.

y_train = tf.keras.utils.to_categorical(y_train, num_classes) 
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

#신경망의 모델을 구축한다.
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(512, activation='sigmoid', input_shape=(784,)))
model.add(tf.keras.layers.Dense(num_classes, activation='sigmoid'))

#구축한 신경망 모델 요약 출력 
model.summary()
sgd = tf.keras.optimizers.SGD(learning_rate=0.1)

#손실 함수를 제곱 오차 함수로 설정하고 학습 알고리즘은 SGD 방식으로 한다. 
model.compile(loss='mean_squared_error',
optimizer=sgd, 
metrics=['accuracy'])

#학습을 수행한다.
history = model.fit(x_train, y_train,
batch_size=batch_size, 
epochs=epochs)

#학습을 평가한다.
score = model.evaluate(x_test, y_test, verbose=0) 
print('테스트 손실값:', score[0])
print('테스트 정확도:', score[1])
</code></pre>
<h3 id="다층--퍼셉트론의--의의">다층  퍼셉트론의  의의</h3>
<p>• 신경망의 기초적인 이론을 정립
• 실용적인 성능
⁻   1980~1990년대에 다층 퍼셉트론은 실용 시스템 제작에 크게 기여
• 인쇄/필기 문자 인식으로 우편물 자동 분류기, 전표 인식기, 자동차 번호판 인식기 등
• 음성 인식, 게임, 주가 예측, 정보 검색, 의료 진단, 유전자 검색, 반도체 결함 검사 등
• 하지만 한계 노출
⁻   잡음이 섞인 상황에서 음성인식 성능 저하 
⁻   필기 주소 인식 능력 저하
⁻   바둑에서의 한계
⁻   딥러닝은 이를 극복함</p>
<h3 id="심층--신경망--dnn-deep-neural-network">심층  신경망  DNN Deep Neural Network</h3>
<p>• DNN은 MLP(다층 퍼셉트론)에서 은닉층의 개수를 증가시킨 것
• &quot;딥(deep)&quot;이라는 용어는 은닉층이 깊다는 것을 의미
• 최근 딥러닝은 컴퓨터 시각, 음성 인식, 자연어 처리, 소셜 네트워크 필터링, 기계 
번역 등에 적용되어서 인간 전문가에 필적하는 결과를 보여줌
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/ac7d9dc5-b523-4808-8199-007dc4d1c01b/image.png" /></p>
<h3 id="딥러닝의--성공--요인">딥러닝의  성공  요인</h3>
<p>• 다수의 은닉층을 가지는 MLP에서의 여러 가지 문제점을 효과적으로 해결
• 문제점
⁻   하드웨어 성능 문제 
⁻   그래디언트 소멸 문제 
⁻   손실 함수 문제
⁻   가중치 초기화 문제 
⁻   최적화 알고리즘 
⁻   과대 적합 문제
⁻   계산 시간 과다 등..</p>
<h3 id="하드웨어--성능--문제">하드웨어  성능  문제</h3>
<p>• DNN의 학습은 매우 계산 집약적이기 때문에 많은 시간과 자원이 많이 요구됨 → 
GPU(Graphic Processing Unit)를 이용하여 하드웨어적 한계를 극복
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/7ae990a2-8926-41f5-bf5c-a095b67e3fd9/image.png" /></p>
<p>• TPU(Tensor Processing Unit)
⁻ 구글에서 2016년 5월에 발표한 데이터 분석 및 딥러닝용 하드웨어, 구글 자체 텐서플로우 소프트 웨어를 이용, 구글은 2015년에 내부적으로 TPU를 사용하기 시작, 2018년 서드파티용으로 판매 시작</p>
<h3 id="그래디언트--소멸--문제">그래디언트  소멸  문제</h3>
<p>• 그래디언트 소멸 문제(gradient vanishing problem)
⁻   특정 활성화 함수를 사용하는 계층이 신경망에 많이 추가되면 손실 함수의 그래디언트가 0에 가까 
워져 학습이 되지 않는 현상
⁻   역전파 학습 알고리즘은 각 층에서의 그래디언트를 이용하여 가중치를 수정</p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/a580a79c-0628-4b64-9f36-d2727aa1987c/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/e84f9495-d1e3-499d-9689-dd7d275bda2c/image.png" /></th>
</tr>
</thead>
</table>
<h3 id="원인-시그모이드-활성화-함수">원인: 시그모이드 활성화 함수</h3>
<p>⁻   아주 큰 양수나 아주 큰 음수가 들어오면 출력이 포화되어 기울기가 0에 수렴</p>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/4df15ff3-4482-41d2-a13b-076f4d559158/image.png" /></p>
<p>• 기존의 MLP는 시그모이드 함수를 활성화 함수로 사용 
→ 새로운 활성화 함수 필요</p>
<h3 id="relu">ReLU</h3>
<p>⁻   f (x) = max(0, x)
⁻   0 이상의 입력 값은 그대로 전달
⁻   미분값은 0이나 1, 0에서는 미분 불가능 
⁻   계산 간단하여 빠르게 계산 가능
⁻   그래디언트 소멸 문제 완화</p>
<h3 id="실습-활성화--함수--실험">[실습] 활성화  함수  실험</h3>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/3bde432d-f12c-4c97-9584-aad517912f7b/image.png" />
• 층을 깊게 하고 활성화 함수를 sigmoid로 설정하면?
<a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.61151&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">실습 사이트</a></p>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/ee344652-d8b5-43fe-a2e5-eb3df4eab9ce/image.png" /></p>
<h3 id="손실--함수--문제">손실  함수  문제</h3>
<p>• 지금까지 손실 함수로 평균 제곱 오차(Mean Squared Error: MSE) 사용</p>
<p>• 손실 함수로 MSE, 노드의 활성화 함수 φ로 시그모이드 함수를 사용하면 저속 수렴 문제(slow convergence) 발생</p>
<p>⁻   목표값 t=0.0, 출력값 z=10.0인 경우? 많이잘못되었다? -&gt; 가중치를 많이 바꿈 
⁻   목표값 t=0.0, 출력값 z=1.0인 경우?  조금잘못되었다? -&gt; 가중치를 조금 바꿈
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/f3bc810c-0df8-40cb-a5ad-3400593c5315/image.png" /></p>
<h3 id="소프트맥스--활성화--함수">소프트맥스  활성화  함수</h3>
<p>• 해결방법: 
출력층 노드의 활성화 함수는 -&gt; 소프트맥스(softmax) 함수, 
손실 함수는 -&gt; 교차 엔트로피(cross entropy) 사용</p>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/1094eedf-df63-45ee-947f-7cbf1fe3224d/image.png" /></p>
<h3 id="교차--엔트로피--손실--함수">교차  엔트로피  손실  함수</h3>
<p>• 교차 엔트로피(cross entropy): 2개의 확률분포 간의 거리를 측정한 것 
⁻   2개의 확률 분포 p(목표 출력), q(실제 출력)에 대해서 다음과 같이 정의
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/505d0fe4-427d-4848-9f67-209862454538/image.png" /></p>
<p>⁻   교차 엔트로피가 크면, 2개의 확률 분포가 많이 다른 것, 작으면 2개의 확률 분포가 거의 일치
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/71dd621f-1d43-417b-98de-6904b820f44a/image.png" /></p>
<h3 id="가중치--초기화--문제">가중치  초기화  문제</h3>
<h4 id="가중치란---얼마나-중요한지-설정해주는-값">가중치란? -&gt; 얼마나 중요한지 설정해주는 값</h4>
<p>• 가중치 초깃값에 따른 각 층의 활성화 값 분포 
⁻   가중치를 표준편차가 1인 정규분포로 초기화할 때
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/b37ae87d-e5f8-4f86-a870-4a70d7e3e704/image.png" /></p>
<p>⁻   각 층의 활성화 값들이 0과 1에 치우쳐 분포 </p>
<h3 id="⁻-시그모이드-함수">⁻ 시그모이드 함수</h3>
<p>• 너무 크거나 작은 값이 들어오면 0과 1에 가까운 값 출력
• 미분(기울기)는 0에 가까워 짐</p>
<h3 id="⁻---기울기-소실gradient-vanishing-문제-발생">⁻   기울기 소실(gradient vanishing) 문제 발생</h3>
<p>• 층이 깊은 딥러닝에서는 더 심각한 문제</p>
<p>w = np.random.randn(node_num, node_num) * 1
↓
w = np.random.randn(node_num, node_num) * 0.1
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/b812d22b-89ba-4ebb-afaf-7ea5e7b788de/image.png" /></p>
<p>⁻   각 층의 활성화 값들이 0.5 부근에 집중 → 기울기 소실은 없으나 표현력이 제한됨
• 다수의 뉴런이 비슷한 값을 출력하면 뉴런을 여러 개 둔 의미가 없어 짐
• 각 층의 활성화 값은 적당히 고루 분포되어 있어야 이상적</p>
<h3 id="xavier-초깃값">Xavier 초깃값</h3>
<p>⁻   딥러닝 프레임워크에서 널리 사용되고 있는 가중치 초깃값 설정 방법
⁻   각 층의 활성화 값들을 광범위하게 분포시킬 목적으로 가중치의 적절한 분포를 찾고자 함 
⁻   앞 계층의 노드가 n개라면 표준편차가 1/sqrt(n)인 분포를 사용
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/1dd936ec-0696-467d-a143-bca887f0541d/image.png" /></p>
<p>node_num = 100  # 앞 층의 노드 수
w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)</p>
<p>⁻   Xavier 초기값을 이용할 때의 각 층의 활성화값 분포</p>
<p>⁻   활성화값이 넓게 분포 → 신경망의 표현력 증가 → 학습 성능 향상</p>
<h3 id="가중치--초기화--방법">가중치  초기화  방법</h3>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/37a2f0d6-5681-4ed7-acc7-982c416b01c1/image.png" /></p>
<h3 id="relu를-활성화-함수로-할-때">ReLU를 활성화 함수로 할 때</h3>
<p>가중치 초깃값에 따른 
활성화 값 분포
⁻   표준편차 0.01
• 각 층의 활성화 값이 아주 작은 값
→ 역전파 때 가중치의 기울기도 작아짐 
→ 학습이 이루어지지 않음
⁻   Xavier 초깃값
• 층이 깊어지면서 치우침이 커짐 
→ 기울기 소실 문제 발생
⁻   He 초깃값
• 모든 층에서 균일하게 분포 
→ 역전파에서도 적절한 값 발생
•  현재의 모범 사례
⁻   ReLU에는 He 초깃값 사용
⁻   sigmoid, tanh 등의 S자 함수에는 
Xavier 초깃값 사용</p>
<h3 id="실습-가중치--초기화--실험">[실습] 가중치  초기화  실험</h3>
<p><a href="https://api.velog.io/rss/!%5B%5D(https:/velog.velcdn.com/images/mi_nini/post/2a69e15a-e07f-437d-b82e-d90ef8ade4cd/image.png)">실습사이트</a>
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/2a69e15a-e07f-437d-b82e-d90ef8ade4cd/image.png" /></p>
<h3 id="미니--배치--mini-batch">미니  배치  Mini-batch</h3>
<p>• 확률적 경사 하강법(Stochastic Gradient Descent, SGD) = 온라인 학습 
⁻   하나의 샘플이 주어지면 오차를 계산하여서 바로 가중치를 변경하는 방법</p>
<h3 id="배치-학습batch-learning">배치 학습(batch learning)</h3>
<p>⁻ 모든 샘플을 모두 보여준 후에 개별 샘플의 그래디언트를 전부 더해서 이것을 바탕으로 모델의 가 
중치를 변경하는 방법</p>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/06a4e86a-c3ec-4ed4-9bff-01a668db9e35/image.png" /></p>
<h3 id="미니--배치--mini-batch-1">미니  배치  Mini-batch</h3>
<p>• 온라인 학습과 배치 학습의 중간에 있는 방법이 미니 배치
⁻   훈련 데이터를 작은 배치들로 분리시켜서 하나의 배치가 끝날 때마다 학습을 수행하는 방법
• 1(온라인) &lt; size(매니 배치) &lt; size(훈련 데이터)</p>
<h3 id="최적화--알고리즘--optimization-algorithm">최적화  알고리즘  Optimization Algorithm</h3>
<p>• 학습률(learning rate)
⁻   한 번에 가중치를 얼마나 변경할 것인가를 결정 
⁻   적절한 학습률은 어떻게 설정할 것인가?
⁻   모든 파라미터에 같은 학습률을 사용할 것인가?
⁻ 지역적 최소값(local minima)에서 어떻게 벗어날 것인가?
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/92bf222c-a1ec-424c-9221-4bb0cc22153f/image.png" /></p>
<h3 id="모멘텀--momentum">모멘텀  Momentum</h3>
<p>• 모멘텀(momentum)
⁻   ‘운동량’을 뜻하는 단어, 기울기 방향으로 물체가 힘을 받아 가속된다는 물리 법칙
⁻   학습 속도를 가속시킬 목적으로 사용 ex -&gt; 가중치의 변화가 많이 되고 있으면 더 많이 가중치를 변화 시키도록 유도
⁻   지역적 최소 문제 해결에도 도움이 됨
• 모멘텀에 의한 최적화 갱신 경로
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/cfc3942b-249a-48da-9cc6-cd0e9f3f68c7/image.png" /></p>
<h3 id="adagrad">AdaGrad</h3>
<p>• 신경망 학습에서 학습률
⁻   너무 작으면 학습 시간이 길어지고, 너무 크면 발산하여 학습 불가
• 학습률 감소(learning rate decay)
⁻   학습을 진행하면서 서서히 학습률을 줄여가는 방법 
⁻   실제로 신경망 학습에서 자주 쓰임
⁻   매개변수 전체의 학습률 값을 일괄적으로 낮추는 방법
• AdaGrad (Adaptive Gradient)
⁻   매개변수 각각의 학습률을 적응적으로 조정하는 방법</p>
<h3 id="adagrad-갱신-방법">AdaGrad 갱신 방법</h3>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/5714dbb8-1ab4-4f6d-82bd-e411d97c3bb2/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/68bcc8bd-5ef7-4657-ad22-a970bafe9efa/image.png" /></th>
</tr>
</thead>
</table>
<p>⁻   h: 기존 기울기 값을 제곱하여 계속 더해줌,     : 행렬의 원소별 곱셈(element-wise) 
⁻   h는 기울기가 클수록 커짐, 기울기가 크다는 것은 갱신이 많이 되었다는 것
⁻   즉, 크게 갱신된 원소의 학습률은 낮아짐</p>
<h3 id="adagrad의-단점">AdaGrad의 단점</h3>
<p>⁻   과거의 기울기를 제곱하여 계속 더해가기 때문에 학습이 진행될수록 갱신 강도가 약해짐 
⁻   실제로 무한히 계속 학습한다면 갱신량은 0에 수렴하여 전혀 갱신되지 않음
• RMSProp
⁻   AdaGrad 단점 개선, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영
⁻   지수이동평균(EMA; Exponential Moving Average)</p>
<h3 id="최적화--알고리즘">최적화  알고리즘</h3>
<p>• 모든 문제에서 항상 뛰어난 기법은 없음
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/29776eb0-26d4-4995-bb88-8cd7164a50b5/image.png" /></p>
<h3 id="과대--적합--overfitting">과대  적합  Overfitting</h3>
<p>• 과대 적합(overfitting)
⁻   모델이 학습 데이터에 특화되어 실제 데이터에서는 좋지 못한 결과가 나오는 현상
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/60523e1e-24b3-4ec3-aca9-52b5a0736326/image.png" /></p>
<h3 id="규제항의-도입regularization">규제항의 도입(regularization)</h3>
<p>⁻   은닉층 수가 많아지거나 가중치가 너무 복잡해지면 과대 적합 발생
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/d746b5d7-a4d7-4379-9532-53cdc676eb69/image.png" /></p>
<h3 id="데이터-증강data-augmentation">데이터 증강(data augmentation)</h3>
<p>⁻   데이터가 부족할 때, 유사한 데이터를 생성하는 방법
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/f5d10b7f-4c66-4ac7-9fe2-731b87eb1a53/image.png" /></p>
<h3 id="드롭--아웃--drop-out">드롭  아웃  Drop out</h3>
<p>• 드롭 아웃(drop out) 
⁻  학습 과정에서 몇 개의 노드들을 랜덤하게 제외하는 것
⁻   제외된 노드를 대신해서 다른 노드들이 특정 데이터를 학습
⁻  해당 데이터에 대한 특징을 많은 노드들이 학습할 수 있도록 유도 
-&gt; 오버피팅을 피하고 결과값이 더 좋아지는 결과가 나온다. 
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/bfb927bc-6978-42eb-9b13-ff3e1d66ea2f/image.png" /></p>
<blockquote>
<h3 id="실습-dnn을--이용한--mnist-숫자인식">[실습] DNN을  이용한  MNIST 숫자인식</h3>
</blockquote>
<pre><code class="language-py">from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.datasets import mnist

#Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

#Normalize the input data
x_train, x_test = x_train / 255.0, x_test / 255.0

#Build the model
model = Sequential()
model.add(Flatten(input_shape=(28, 28)))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))

#Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])

#Display the model's architecture
model.summary()

#Train the model
model.fit(x_train, y_train, epochs=5)

#Evaluate the model
model.evaluate(x_test, y_test)
</code></pre>