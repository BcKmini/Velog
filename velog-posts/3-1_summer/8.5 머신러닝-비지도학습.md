<blockquote>
<h2 id="📝목차">📝목차</h2>
<p>Chapter 1. 비지도 학습 개요
Chapter 2. 차원 축소
Chapter 3. 클러스터링
Chapter 4. 종합실습 : 고객 세분화</p>
</blockquote>
<hr />
<h2 id="📌span-stylecolorindianred1-비지도-학습-개요span">📌<span style="color: indianred;">1. 비지도 학습 개요</span></h2>
<h3 id="전체-프로세스crisp-dm">전체 프로세스(CRISP-DM)</h3>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/de9f8a72-f27c-4017-b324-d5c742c07a0f/image.png" /></p>
<hr />
<h2 id="모델-모델링">모델, 모델링</h2>
<p>✓ Model</p>
<ul>
<li>모델 : 데이터로부터 패턴을 찾아, 수학식으로 정리해 놓은 것</li>
<li>모델링 : 가능한한 오차가 적은 모델을 만드는 과정
✓ 모델의 목적</li>
<li>샘플을 가지고 전체를 추정</li>
<li>샘플 : 표본, 부분집합, 일부, 과거의 데이터(우리가 들고 있는 데이터!)</li>
<li>전체 : 모집단, 전체집합, 현재와 미래의 데이터
•  추정 : 예측, 추론</li>
</ul>
<h2 id="패턴을-찾는-방법---지도학습">패턴을 찾는 방법 - 지도학습</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/5fbcfc19-bb1d-4fac-9a3f-ec273e922c13/image.png" /></p>
<h2 id="패턴을-찾는-방법---비지도학습">패턴을 찾는 방법 - 비지도학습</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/bb885f34-ccbc-47ac-a94d-fc4c435e3b3d/image.png" /></p>
<hr />
<h2 id="패턴을-찾는-방법">패턴을 찾는 방법</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/aeff2d3b-a76d-4f64-8db2-a63fa27a2a60/image.png" /></p>
<ol>
<li>a와 b를 나누자</li>
<li>여기 까지를 normal로 정하자</li>
<li>유사한 것 끼리 묶자<h2 id="📌비지도-학습-특징">📌비지도 학습 특징</h2>
✓ 학습 시 x만 사용</li>
</ol>
<ul>
<li>x 안에서 패턴 인식 문제
✓ 후속 작업 필요(비지도 학습으로 끝나지 않음)</li>
<li>[차원축소] 고차원 데이터를 축소하여 새로운 feature를 생성 ➔ 시각화, 지도학습 연계</li>
<li>[클러스터링] 고객별 군집 생성 ➔ 고객 집단의 공통 특성 도출을 위한 추가 분석</li>
<li>[이상탐지] 정상 데이터 범위 지정 ➔ 범위 밖 데이터를 이상치로 판정</li>
<li>*</li>
</ul>
<hr />
<h1 id="span-stylecolorindianred📌차원-축소span"><span style="color: indianred;">📌차원 축소</span></h1>
<h2 id="차원의-저주">차원의 저주</h2>
<p>변수가 많을수록 모델 성능 향상?
✓ 차원(dimension)</p>
<ul>
<li>차원은 여러가지 의미로 사용되지만,</li>
<li>여기서는 차원의 수 = 변수(feature)의 수
✓ 다양한 변수를 고려 ➔ 모델 성능 향상</li>
<li>예, 고객의 건강상태 분석</li>
<li>키, 몸무게 (2차원) ➔ 기본적인 분석</li>
<li>+혈압, 체성분 지수, 나이 (5차원) : ➔ 더 구체적인 건강상태 분석 가능</li>
</ul>
<h2 id="변수가-많을수록-모델-성능-향상">변수가 많을수록 모델 성능 향상?</h2>
<p>✓ 이어서 더 많은 변수 수집</p>
<ul>
<li>성별, 출신지역, 소득수준, 자녀 수, 주거형태</li>
</ul>
<p>✓ 변수를 계속 추가하다 보니…</p>
<ul>
<li>꼭 필요한 데이터가 아닌데 포함되기도 함</li>
<li>오히려 방해가 될 수도</li>
<li>불필요하게 복잡한 모델</li>
<li>더 큰 문제는, 데이터가 굉장히 희박해 집니다!</li>
</ul>
<hr />
<hr />
<h2 id="희박한sparse-데이터">희박한(sparse) 데이터</h2>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/a0997d63-afb2-4d2f-8f3f-69290e36c2d2/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/2619676b-bbda-4e1e-be70-b5dbcec0b427/image.png" /></th>
</tr>
</thead>
</table>
<p>✓ 변수가 많아지면</p>
<ul>
<li>조건(작은 키, 가벼운 몸무게, 저혈압)에 맞는 데이터 희박
➔ 학습이 적절하게 되지 않을 가능성 높아짐.
✓ 이를 차원의 저주이라고 부릅니다.</li>
<li>많다고 꼭 좋은 건 아니...</li>
</ul>
<p>✓ 고차원 문제의 본질 : [희박한 데이터 문제]
✓ [희박한 데이터 문제] 해결방안</p>
<ul>
<li>① 행을 늘리기. ➔   데이터 늘리기</li>
<li>② 열을 줄이기. ➔   차원 축소</li>
</ul>
<hr />
<h2 id="해결방안--차원-축소">해결방안 : 차원 축소</h2>
<p>✓ 차원 축소
▪   다수의 feature (고차원) ➔ 새로운 소수의 feature (저차원)로 축소
▪   기존 특성을 최대한 유지</p>
<p>✓ 대표적인 방법 : 주성분 분석(PCA), t-SNE</p>
<hr />
<h1 id="📌span-stylecolorindianred차원-축소---주성분-분석pcaspan">📌<span style="color: indianred;">차원 축소 - 주성분 분석(PCA)</span></h1>
<h2 id="차원을-줄이는-방법">차원을 줄이는 방법</h2>
<p>✓   주성분 분석(PCA Principal Component Analysis)</p>
<ul>
<li>변수(차원)의 수보다 적은 저차원의 평면으로 투영 Projection</li>
<li>평면1, 평면2 중 럭비 공의 특징을 잘 반영한 평면은 무엇일까요? (어떤 그림자가 럭비공과 비슷하나요?)
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/44fbb3e2-b28b-4b6f-af7c-6030fc4f22b0/image.png" /></li>
</ul>
<h2 id="주성분-분석-pca">주성분 분석 PCA</h2>
<p>✓ 어떤 평면으로 투영시킬 것인가?</p>
<ul>
<li>정보의 특성을 최대한 유지하면서</li>
<li>차원을 축소</li>
<li><ul>
<li>분산을 최대한 유지
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/628ef3f8-eb4e-4b0b-bc7d-09c8561ac3be/image.png" /></li>
</ul>
</li>
<li>맨 위_  분산이 가장 큼</li>
<li>맨 아래_ 분산이 가장 작음</li>
</ul>
<h3 id="✓-pca-절차">✓ PCA 절차</h3>
<p>① 학습 데이터셋에서 분산이 최대인 첫번째 축(axis)을 찾음.
② 첫번째 축과 직교(orthogonal)하면서 분산이 최대인 두 번째 축을 찾음. 
③ 첫 번째 축과 두 번째 축에 직교하고 분산이 최대인 세 번째 축을 찾음. 
④ ①~③과 같은 방법으로 데이터셋의 차원(feature 수) 만큼의 축을 찾음.</p>
<p>✓ 주성분 분석을 수행하면…</p>
<ul>
<li>각 축의 단위벡터를 주성분이라고 부르고,</li>
<li>각 축 별 투영된 값이 저장됨</li>
</ul>
<h3 id="pca-사용하기">PCA 사용하기</h3>
<p>✓ 전처리 : 스케일링필요</p>
<ul>
<li>주성분 결정시, 분산 비교(크기 비교)</li>
<li>스케일링 없이 PCA를 수행 ➔ 스케일이 가장 큰 변수에 영향을 가장 많이 받게 됨.</li>
</ul>
<h3 id="✓-pca-문법">✓ PCA 문법</h3>
<p>▪   선언 : 주성분의 개수(n) 지정
•  1 ≤ n ≤  전체 feature의 수
▪   적용
•  x_train : fit &amp; transform
•  다른 데이터셋 : transform
•  결과형태 : numpy array</p>
<pre><code class="language-py"># 주성분   분석    선언
pca = PCA(n_components=n)
# 만들고, 적용하기
x_train_pc = pca.fit_transform(x_train) 
x_val_pc = pca.transform(x_val)</code></pre>
<h2 id="✓-주성분의-개수-정하기">✓ 주성분의 개수 정하기</h2>
<p>▪   주성분의 개수를 늘려가면서
▪   원본 데이터 분산과 비교
▪   Elbow Method
▪   .explained_variance_ratio_
원본데이터의 전체  분산  대비 
누적  주성분의 차이  비율
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/6ad0a7e7-0182-44ca-95d1-17c3e61cf9d5/image.png" /></p>
<pre><code class="language-py">plt.plot(range(1,n+1), pca.explained_variance_ratio_,
marker = '.')
plt.xlabel('No. of PC')
plt.grid()
plt.show()</code></pre>
<hr />
<h2 id="elbow-method">Elbow Method</h2>
<p>✓ 팔꿈치 지점 근방에서 적절한 값을 찾아라.</p>
<ul>
<li>X축의 값이 계속 증가해도, y축의 값의 개선 폭이 줄어 드는 지점</li>
<li>Trade-Off 관계일 때, 적절한 지점을 찾기 위한 휴리스틱 방법.<blockquote>
<p>휴리스틱 법(heuristics) : 불충분한 시간이나 정보로 인하여 합리적인 판단을 할 수 없거나, 체계적이면서 합리적인 판단이 굳이 필요하지 않은 상황에서 사람들이 빠르게 사용할 
수 있게 보다 용이하게 구성된 간편추론의 방법</p>
</blockquote>
</li>
</ul>
<hr />
<h1 id="📌span-stylecolorindianred차원-축소---추가t-snespan">📌<span style="color: indianred;">차원 축소 - ([추가]t-SNE)</span></h1>
<h2 id="pca의-단점">PCA의 단점</h2>
<p>✓ 아래 2차원 데이터를 1차원으로 투영 한다면…
✓ PCA → 선형 축소 방식</p>
<ul>
<li>분산의 크기만 고려한 선형축소방식</li>
<li>저차원에서 특징을 잘 담아내지 못하는 
경우 발생
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/1a5627f8-fe29-4c56-a9e4-5d6e79f25efd/image.png" /></li>
</ul>
<h3 id="t-sne-t-distributed-stochastic-neighbor-embedding">t-SNE t-distributed Stochastic Neighbor Embedding</h3>
<p>✓ 원본의 특성을 최대한 살리면서 축소하는 방법이 필요!</p>
<ul>
<li>① 원본에서 가까운 거리의 점들은  ➔ 원본의 유사도 맵 생성</li>
<li>② 축소한 후에도 가깝게 만들자.    ➔ 축소된 데이터의 유사도 맵
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/40e6d8ca-8940-4588-b0da-de5097ad11ad/image.png" /></li>
</ul>
<hr />
<h2 id="t-sne-원리">t-SNE 원리</h2>
<p>✓ 원본 데이터에서 유사도 맵 만들기</p>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/b031382b-41a2-469b-b96d-fd80a78349d3/image.png" /></p>
<p>✓ 저 차원 데이터에서 유사도 맵 만들기
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/79910c19-0c45-4946-8425-ea856995bfef/image.png" /></p>
<h2 id="t-sne-사용하기">t-SNE 사용하기</h2>
<p>✓ 전처리 : 스케일링</p>
<ul>
<li>꼭 필요한 것은 아니지만, 해주는 것을 권장
✓ 학습</li>
<li>고차원을 2 ~ 3 차원으로 축소하여</li>
<li>주로 데이터 시각화를 위해 사용됨</li>
<li>학습하는데 오래 걸림<pre><code class="language-py">from sklearn.manifold import TSNE 
# 2차원으로   축소하기
tsne = TSNE(n_components = 2) 
x_tsne = tsne.fit_transform(x)</code></pre>
</li>
</ul>
<hr />
<h1 id="📌span-stylecolorslateblue요약---summaryspan">📌<span style="color: slateblue;">요약 |  Summary</span></h1>
<h3 id="focus">Focus</h3>
<p>▪ 차원을 축소하여 새로운 feature 만들기
▪ 활용
• 고차원 데이터 시각화
• 지도학습으로 연계</p>
<h3 id="pca">PCA</h3>
<p>▪ 고차원의 특징(분산)을 최대한 유지
▪ 선형 방식
▪ Feature의 수 만큼 추출 가능</p>
<h3 id="tsne">tSNE</h3>
<p>▪ 고차원의 특징(유사도)를 최대한 유지
▪ 비선형 방식
▪ 2~3개로 축소 ➔ 시각화에 자주 사용됨</p>
<hr />
<h1 id="📌span-stylecolorindianred3-군집화span">📌<span style="color: indianred;">3. 군집화</span></h1>
<h2 id="k-means">K-means</h2>
<h3 id="k-means-개념">k-means 개념</h3>
<p>✓ 개념</p>
<ul>
<li>K개의 평균으로 부터 거리를 계산하고,</li>
<li>가까운 평균으로 묶어</li>
<li>Cluster를 나누는 방식
✓ 절차</li>
<li>오른쪽 애니메이션을 반복해서 보면서, 
어떤 절차로 진행되는지 상상해 봅시다.
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/d15d0796-6e45-4459-b813-73f966aad5c1/image.png" />
✓   ① 클러스터의 개수 지정(k)</li>
<li>군집화 할 (그룹의) 수로 지정하는 것을 권장
✓   ② 그룹의 중심 점(mean)이 무작위로 선택됨
✓   ③ 임의로 선택된 중심 점과 각 점 간의 거리를 계산해서 
가장 가까운 중심점의 그룹(군집)으로 선택됨
✓ ④ 선택된 그룹의 점들을 기준으로 중심점을 계산해서 찾고,
✓   ⑤ ③~④를 반복</li>
<li>중심점의 변화가 거의 없을 때까지 진행.</li>
</ul>
<hr />
<hr />
<h2 id="k-means-문법">k-means 문법</h2>
<p>✓ KMeans 함수 사용</p>
<ul>
<li>k : n_clusters</li>
<li>n_init</li>
<li>auto : 데이터 크기와 군집 수에 맞게 
적절하게 지정(권장사항)</li>
<li>학습할 때는 x만 입력</li>
<li>예측</li>
<li>지정한 클러스터의 개수 내에서 구분</li>
</ul>
<pre><code class="language-py"># k means 학습
model = KMeans(n_clusters= 2, n_init = 'auto') 
model.fit(x)
# 예측
pred = model.predict(x) 
print(pred)</code></pre>
<hr />
<h2 id="k-means에서-적정-k-값-찾기①">k-means에서 적정 k 값 찾기①</h2>
<p>✓ 클러스터의 개수(k)를 어떻게 지정해 줄 것인가.
✓ Inertia value </p>
<ul>
<li>군집화가 된 후에, 각 중심점에서 군집의 데이터 간의 거리를 합산한 값.
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/7fd1d75e-2b53-4ade-a67b-6d1390703b4d/image.png" /></li>
</ul>
<p>✓ 클러스터의 개수(k)를 증가 시키면서
✓ Inertia value 를 뽑고
✓ 적정 k 값 찾기</p>
<ul>
<li>Elbow Method!
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/32e2a9fb-35f3-4701-af2a-d553fb2399d9/image.png" /></li>
</ul>
<h2 id="k-means에서-적정-k-값-찾기②">k-means에서 적정 k 값 찾기②</h2>
<p>✓ 실루엣 점수</p>
<ul>
<li>결과의 품질 측정 지표</li>
<li>-1 ~ 1 사이 값</li>
<li>실루엣 점수를 계산하려면
최소한 k가 2개 이상이어야 함.</li>
</ul>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/04527926-190b-401e-b9f6-000c357d12b5/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/2bcf09dc-a1ff-4549-b878-0edaf34fd84f/image.png" /></th>
</tr>
</thead>
</table>
<hr />
<h2 id="k-means의-단점">k-means의 단점</h2>
<p>✓ k-means는 중심점에서의 거리로 군집화</p>
<ul>
<li>덩어리 형태에서만 군집이 잘 형성됨.</li>
<li>오른쪽과 같은 형태에서는…
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/5a0249b0-4d86-45c6-999d-ea0399e0fcf5/image.png" /><blockquote>
<p> 이러한 단점을 보완하는 알고리즘 ➔ DBSCAN </p>
</blockquote>
</li>
</ul>
<hr />
<h1 id="📌span-stylecolorindianred군집화---추가dbscanspan">📌<span style="color: indianred;">군집화 - [추가]DBSCAN</span></h1>
<h2 id="density-based-spatial-clustering-of-applications-with-noise">Density-Based Spatial Clustering of Applications with Noise</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/04ad852e-8634-4f08-8ac5-093c60e16319/image.png" /></p>
<h2 id="dbscan-절차">DBSCAN 절차</h2>
<p>① 임의의 한 점으로 부터 시작
② 반경 범위내에 최소 포인트 수가 존재하는지 확인
③ 존재한다면 각 포인트 들을 중심으로 다시 원을 그어 최소 포인트 수 확인
④ ② ~ ③ 반복수행
⑤ 존재하지 않으면, 군집에 포함되지 않은 점으로 이동하여 ① ~ ④ 반복수행
⑥ 어느 군집에도 포함되지 않는 점은 이상치로 간주함.</p>
<h3 id="dbscan도-epsilon-을-지정해-줘야-한다">DBSCAN도 epsilon 을 지정해 줘야 한다</h3>
<p>✓ KNN의 알고리즘으로부터</p>
<ul>
<li>각 점과 근처 n개 점과의 평균 거리 계산</li>
<li>거리 순으로 정렬하여 그래프 그리기</li>
<li>급격히 멀어지기 시작하는
거리 구간을 찾아 eps 값으로 적용</li>
<li>elbow method! 
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/4702ede1-7e7f-4635-b29d-aa0e52ddf7ab/image.png" /></li>
</ul>
<hr />
<h1 id="📌span-stylecolorslateblue요약---summaryspan-1">📌<span style="color: slateblue;">요약 |  Summary</span></h1>
<p>✓ K-means</p>
<ul>
<li>K개의 평균으로부터 가까운 데이터를 묶음</li>
<li>평균을 조절하면서 최적의 평균값 위치를 찾아간다.</li>
<li>단점 : 덩어리 형태로 묶을 수 있어야 함. K값은 사람이 정해줘야 함.
✓ DBSCAN</li>
<li>연쇄적으로 근처의 데이터들을 클러스터 안에 포함시킴.</li>
<li>덩어리 형태가 아니어도 해결 가능.</li>
<li>단점 : 연쇄적으로 묶을 반경을 사람이 정해줘야 함.</li>
</ul>
<hr />
<h1 id="📌span-stylecolorindianred4-종합실습-고객-세분화span">📌<span style="color: indianred;">4. 종합실습: 고객 세분화</span></h1>
<h2 id="쇼핑몰-고객-분석">쇼핑몰 고객 분석</h2>
<p>✓ 쇼핑몰 고객들의 정보를 기반으로 
고객을 세분화해 봅시다.</p>
<p>✓ 데이터 정보</p>
<ul>
<li>고객ID</li>
<li>성별 :   Female, Male</li>
<li>나이</li>
<li>연간 소득 : 단위 1000 $</li>
<li>구매 점수 : 1 ~ 100 점</li>
</ul>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/2f800672-06b7-44d0-8d37-b71f49bfc720/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/36c958d6-441b-49ca-8c8d-91b74b7e2aa0/image.png" /></th>
</tr>
</thead>
</table>
<p>✓ 군집</p>
<ul>
<li>k-means 이용</li>
<li>k 는 3~6개 사이에서 elbow method로 결정</li>
<li>너무 크면 군집 후 사후 분석 어려움
✓ 사후 분석</li>
<li>① 군집 결과를 label(y)로 간주</li>
<li>② 각 군집(고객 군)의 비즈니스 특징 도출</li>
<li>시각화를 이용하여 특징 도출 ➔ sns.pairplot</li>
<li>③ 각 고객 군에 대한 마케팅 전략 수립</li>
</ul>
<hr />
<blockquote>
<h2 id="코드-참고--github--7주차85">코드 참고  <a href="https://github.com/BcKmini/Ai_Python">Github</a> : 7주차/8.5</h2>
</blockquote>