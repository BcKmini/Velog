<blockquote>
<h2 id="목차">목차</h2>
</blockquote>
<h3 id="인공신경망-기초-퍼셉트론perceptron의-이해">인공신경망 기초: 퍼셉트론(Perceptron)의 이해</h3>
<p>⁻   인공신경망의 역사, 특징
⁻   퍼셉트론의 구조 및a 동작 
⁻   퍼셉트론의 구현</p>
<h3 id="다층-퍼셉트론multi-layer-perceptron">다층 퍼셉트론(Multi-Layer Perceptron)</h3>
<p>⁻   퍼셉트론의 한계
⁻   특징 공간 변환을 이용한 다층 퍼셉트론</p>
<h2 id="1-인공신경망-artificial-neural-network">1. 인공신경망 Artificial Neural Network</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/9543afb2-1c59-45bc-9ec2-e4da75b63e1d/image.png" /></p>
<h3 id="사람의-뉴런">사람의 뉴런</h3>
<p>⁻   두뇌의 가장 작은 정보처리 단위
⁻   수상돌기dendrite는 신호 수신
⁻   세포체는cell body 간단한 연산
⁻   축삭은axon 처리 결과를 전송
⁻ 사람은 1011개 정도의 뉴런을 가지며, 뉴런 
은 1000개 가량 다른 뉴런과 연결되어 있어 
1014개 정도의 연결</p>
<h3 id="인공신경망">인공신경망</h3>
<p>⁻   기계 학습 역사에서 가장 오래된 기계 학습 
모델이며, 현재 가장 다양한 형태를 가짐
⁻   딥 러닝의 기초</p>
<h3 id="인공신경망의-간략한-역사">인공신경망의 간략한 역사</h3>
<p>•  1943년 매컬럭과 피츠의 최초의 신경망
•  1949년 헤브는 최초로 학습 알고리즘 제안
•  1958년 로젠블렛은 퍼셉트론 제안
•  위드로와 호프의 Adaline 과 Madaline
•  1960년대의 과대 평가
•  1969년 민스키와 페퍼트의 저서 『Perceptrons』는 퍼셉트론의 한계를 수학적으로 입증 
⁻    퍼셉트론은 선형분류기에 불과하여 XOR 문제조차 해결 못함
•  신경망 연구 퇴조
•  1986년 루멜하트의 저서 『Parallel Distributed Processing』은 다층 퍼셉트론 제안
•  신경망 연구 부활
•  1990년대 SVM에 밀리는 형국
•  2010년대 딥러닝이 실현되어 신경망이 기계 학습의 주류 기술로 자리매김
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/b1084cff-7518-450e-86a2-3f9befae15c7/image.png" /></p>
<h3 id="인공신경망의--특징">인공신경망의  특징</h3>
<p>• 학습이 가능함
⁻   데이터만 주어지면 신경망은 예제로부터 배울 수 있음
• 몇 개의 소자가 오작동 하더라도 전체적으로는 큰 문제가 발생하지 않음</p>
<h3 id="퍼셉트론-perceptron-기본신경망-구조">퍼셉트론 perceptron (기본신경망 구조)</h3>
<p>• 퍼셉트론 (입력을 여러개 받아 출력함)
⁻   1957년 로젠블라트(Frank Rosenblatt) 
⁻   가장 원시적 신경망
⁻   생물학적 뉴런을 수학적인 모델로 만듦
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/3149f150-3c74-4e6a-a0d1-4a733d52316e/image.png" />
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/dcd0199c-c1f4-4215-ac55-6a06942ff3a4/image.png" />
⁻   노드, 가중치, 층과 같은 새로운 개념을 도입하고 학습 알고리즘을 창안함
⁻   딥러닝을 포함한 현대 신경망은 퍼셉트론을 병렬과 순차 구조로 결합하여 만듦 
→ 현대 신경망의 중요한 구성 요소
용어(가중치 등) + 이해가 중요함 </p>
<h3 id="퍼셉트론의--구조">퍼셉트론의  구조</h3>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/edd1e20e-c150-4e42-8780-705e604dba42/image.png" /></p>
<p>• 입력층과 출력층을 가짐
⁻ 입력층은 연산을 하지 않으므로 퍼셉트론은 단일 층 구조라고 간주 
-&gt;Single-layer Perceptron
• 입력층의 i번째 노드는 특징 벡터 𝐱𝐱 = 𝑥𝑥1 , 𝑥𝑥2 , ⋯ , 𝑥𝑥𝑑𝑑 T 의 요소 𝑥𝑥𝑖𝑖 를 담당
• 항상 1이 입력되는 바이어스 노드 -&gt; 그림에서 입력층 첫번째 노드 
• 출력층은 한 개의 노드
• i번째 입력층 노드와 출력층을 연결하는 에지는 가중치 𝑤𝑤𝑖𝑖 를 가짐</p>
<h3 id="퍼셉트론의--동작">퍼셉트론의  동작</h3>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/b82c0d5e-e456-4280-8cbf-d83313ec76f5/image.png" /></p>
<h3 id="활성화-함수">활성화 함수</h3>
<p>Activation function = 계단 함수(Python)
• 뉴런에서는 입력 신호의 가중치 합이 어떤 임계값을 넘는 경우에만 뉴런이 활성 
화되어서 1을 출력, 그렇지 않으면 0을 출력
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/3704ef03-898d-4703-9932-9bb62e139962/image.png" /></p>
<h4 id="-2차원-특징-벡터로-표현되는-샘플을-4개-가진-훈련-집합-논리-연산-or">• 2차원 특징 벡터로 표현되는 샘플을 4개 가진 훈련 집합 (논리 연산 OR)</h4>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/6abf0899-884c-4309-a4da-084ecc49f45f/image.png" /></p>
<p>• 아래의 퍼셉트론은 위의 데이터를 분류하는가?
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/c8d603b6-13d5-4c69-b560-b45394ad4349/image.png" />
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/438d9cce-1cfa-491e-adb6-d910f71f0d89/image.png" /></p>
<h4 id="-퍼셉트론을-기하학적으로-설명하면">• 퍼셉트론을 기하학적으로 설명하면</h4>
<p>• 𝑤𝑤1 과 𝑤𝑤2 는 직선의 방향, 𝑤𝑤0 은 절편을 결정
• 결정 직선은 전체 공간을 +1과 -1의 두 부분공간으로 분할하는 분류기 역할
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/6d818a64-08a6-409e-a240-8ddd17feb1d1/image.png" /></p>
<p>⁻   d 차원 공간에서는
• 2차원은 결정 직선, 3차원은 결정 평면, 4차원 이상은 결정 초평면
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/3ea2143c-534c-4c5e-8c10-c9a78efacbee/image.png" /></p>
<p>퍼셉트론에서  학습이란?
• 지금까지는 학습을 마친 퍼셉트론을 가지고 동작을 설명한다</p>
<h4 id="중요포인트----𝑤𝑤0---𝑤𝑤1---𝑤𝑤2-가-어떤-값을-가져야-100-옳게-분류할까--고민하기">중요포인트 -&gt;  𝑤𝑤0 ,  𝑤𝑤1 ,  𝑤𝑤2 가 어떤 값을 가져야 100% 옳게 분류할까? -&gt;고민하기</h4>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/fc710b0a-27cd-449f-b998-f0c2ce55d2cd/image.png" /></p>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/1e941150-ad02-4846-a733-12d766c8dd1f/image.png" /></p>
<h3 id="고민해보기-실습-퍼셉트론을--이용한--and-연산--구현">고민해보기, [실습] 퍼셉트론을  이용한  AND 연산  구현</h3>
<h4 id="적절한-w0-w1-w2-값은-무엇인가">적절한 w0, w1, w2 값은 무엇인가?</h4>
<h3 id="퍼셉트론--학습--알고리즘-x--입력-d--정답-y--예측값">퍼셉트론  학습  알고리즘 x-&gt; 입력 d-&gt; 정답 y-&gt; 예측값</h3>
<p>Input: 학습 데이터(x1, d1), …, (x^m, d^m)</p>
<ol>
<li>모든 가중치(w)들과 바이어스(b)를 0 또는 작은 난수로 초기화</li>
<li>while (가중치가 변경되지 않을 때까지 반복, 또는 일정 횟수, 또는 일정 오차 이하 등)</li>
<li>각 학습 데이터 x^k와 정답 d^k에 대하여</li>
<li>y^k(t) = f(w•x)</li>
<li>if d^k==y^k(t)</li>
<li>continue</li>
<li>else -&gt; 가중치를 업데이트 해줘라</li>
<li>모든 가중치 wi에 대하여 wi(t+1) = wi(t) + α•(d^k-y^k(t))•xi^k<pre><code>                   #가중치             #오차</code></pre></li>
</ol>
<h3 id="퍼셉트론의--and-연산--학습--과정">퍼셉트론의  AND 연산  학습  과정</h3>
<p>• w=0, b=0, 학습률(α)=0.1                   wi(t+1) = wi(t) + α•(dk-yk(t))•xik
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/544d8c18-4632-49b4-8e80-ddb99545e51c/image.png" /></p>
<blockquote>
<h3 id="퍼셉트론--프로그래밍">퍼셉트론  프로그래밍</h3>
</blockquote>
<pre><code class="language-py"># 뉴론의 출력 계산 함수  
def calculate(input):
    global weights 
    global bias
    activation = bias    # 바이어스
    for i in range(2):    # 입력신호 총합 계산 
        activation += weights[i] * input[i]
    if activation &gt;= 0.0:    # 스텝 활성화 함수 
        return 1.0 
    else: 
        return 0.0</code></pre>
<blockquote>
</blockquote>
<pre><code class="language-py"># 학습  알고리즘
def train_weights(X, y, l_rate, n_epoch): 
    global weights
    global bias
    for epoch in range(n_epoch):   # 에포크  반복 
        sum_error = 0.0
        for row, target in zip(X, y):  # 반복
            actual = calculate(row)    # 실제  출력  계산 
            error = target - actual      # 오류  계산 
            bias = bias + l_rate * error
            sum_error += error**2      # 오류  제곱  계산 
            for i in range(2):              # 가중치  변경
                weights[i] = weights[i] + l_rate * error * row[i]
                print(weights, bias)
            print('에포크  번호=%d, 학습률=%.3f, 오류=%.3f' % (epoch, l_rate, sum_error)) 
        return weights</code></pre>
<pre><code class="language-py"># AND 연산 학습 데이터셋, 샘플과 레이블이다. 
X = [[0,0],[0,1],[1,0],[1,1]]
y = [0, 0, 0, 1]
# 가중치와 바이어스 초기값 
weights = [0.0, 0.0] 
bias = 0.0
l_rate = 0.1                # 학습률
n_epoch = 5                # 에포크 횟수
weights = train_weights(X, y, l_rate, n_epoch) 
print(weights, bias)</code></pre>
<blockquote>
<h3 id="sklearn으로--퍼셉트론--실습하기">sklearn으로  퍼셉트론  실습하기</h3>
</blockquote>
<pre><code class="language-py">from sklearn.linear_model import Perceptron 
# AND 연산 샘플과 레이블이다.
X = [[0,0],[0,1],[1,0],[1,1]] 
y = [0, 0, 0, 1]
# 퍼셉트론 생성. tol는 종료 조건, random_state는 난수의 시드 
clf = Perceptron(tol=1e-3, random_state=0)
# 학습을 수행한다. 
clf.fit(X, y)
# 테스트를 수행한다. 
print(clf.predict(X)) # 실제 출력값 
# 정확도 테스트 
print(clf.score(X, y))</code></pre>
<blockquote>
<h3 id="실습perceptron를-이용한-iris-데이터-분류">[실습]Perceptron를 이용한 iris 데이터 분류</h3>
</blockquote>
<pre><code class="language-py">from sklearn.linear_model import Perceptron
from sklearn.datasets import load_iris
iris = load_iris()
# 학습, 테스트 데이터 분할 7:3
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, 
random_state=0)
clf = Perceptron(tol=1e-3, random_state=0) 
# 학습을 수행한다.
clf.fit(X_train, y_train) 
# 테스트를 수행한다.
print(clf.predict(X_test))
# 정확도가 얼마 나오는지 확인해보자! 
print(clf.score(X_test, y_test))</code></pre>
<h2 id="2-퍼셉트론의--한계">2. 퍼셉트론의  한계</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/3fa3a020-5948-45e2-959a-5a51ceed3261/image.png" />
• 퍼셉트론은 선형 분류기라는 한계
⁻   선형 분리 불가능한 상황에서는 일정한 양의 오류 
⁻   예) XOR 문제에서는 75%의 정확도 한계</p>
<p>⁻   민스키의 『Perceptrons』
• 퍼셉트론의 한계를 지적하고 다층 구조를 이용한 극복 방안 제시. 당시 기술로 실현 불가능
• 1974년 웨어보스는 박사 논문에서 오류 역전파 알고리즘 제안
• 1986년 룸멜하트의 저서 『Parallel Distributed Processing』 다층 퍼셉트론 이론 정립하여 신경망 부활</p>
<blockquote>
<h3 id="퍼셉트론의--xor-데이터--분류">퍼셉트론의  XOR 데이터  분류</h3>
<pre><code class="language-py">from sklearn.linear_model import Perceptron 
#XOR 연산 샘플과 레이블이다.
X = [[0,0],[0,1],[1,0],[1,1]] 
y = [0, 1, 1, 0]
퍼셉트론 생성. tol는 종료 조건, random_state는 난수의 시드 
clf = Perceptron(tol=1e-3, random_state=0)
학습을 수행한다. 
clf.fit(X, y)
테스트를 수행한다. 
print(clf.predict(X))
#정확도 테스트 
print(clf.score(X, y))</code></pre>
</blockquote>
<pre><code>
![](https://velog.velcdn.com/images/mi_nini/post/f4a90a9a-c8dc-4b10-bf8f-14ce5734247a/image.png)
velog.io/5a24304c-9601-484c-ae37-0034a4fe56e2)
### 퍼셉트론  2개를  사용한  XOR 문제의  해결
• 퍼셉트론 ①과 퍼셉트론 ②가 모두 +1이면 ●, 그렇지 않으면 □로 분류

![](https://velog.velcdn.com/images/mi_nini/post/5aba2442-e421-4c46-add9-0ecf7b0c4ca5/image.png)


### 특징  공간  변환
• 퍼셉트론 2개를 병렬로 결합하면,
⁻ 원래 공간 𝐱𝐱  =   𝑥𝑥1 , 𝑥𝑥2     T 를 새로운 특징 공간 𝐳𝐳  =  𝑧𝑧1 , 𝑧𝑧2     T 로 변환 
⁻ 새로운 특징 공간 𝐳𝐳에서는 선형 분리 가능함

![](https://velog.velcdn.com/images/mi_nini/post/9946f264-ef5a-444f-9937-7d21ef8a2058/image.png)


• 퍼셉트론 1개를 순차 결합하면?
⁻ 새로운 특징 공간 𝐳𝐳에서 선형 분리를 수행하는 퍼셉트론 ③을 순차 결합하면 다층 퍼셉트론

&gt; ### 다층  퍼셉트론의  XOR 데이터  분류
```py
from sklearn.neural_network import MLPClassifier 
#XOR 연산 샘플과 레이블이다.
X = [[0,0],[0,1],[1,0],[1,1]] 
y = [0, 1, 1, 0]
다층 퍼셉트론 생성.
다양한 매개변수(hidden_layer_sizes, activation, solver, learning_rate_init 등)를 이용해 MLP 구성 
mlp = MLPClassifier()
#학습을 수행한다. 
mlp.fit(X, y)
#테스트를 수행한다. 
print(mlp.predict(X))
#정확도 테스트 
print(mlp.score(X, y))</code></pre>