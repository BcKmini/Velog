<h3 id="ğŸ“ëª©ì°¨">ğŸ“ëª©ì°¨</h3>
<h3 id="í•©ì„±ê³±-ì‹ ê²½ë§-convolutional-neural-network">í•©ì„±ê³± ì‹ ê²½ë§ Convolutional Neural Network</h3>
<p>â»  CNN ì•„ì´ë””ì–´ì™€ ê¸°ìˆ 
â€¢ í•©ì„±ê³± ì¸µ
â€¢ Paddingê³¼ Stride
â€¢ í’€ë§ ì¸µ</p>
<p>â»  CNN êµ¬ì¡°</p>
<p>â»  DNNê³¼ CNN ë¹„êµ</p>
<p>How we're teaching computers to understand pictures? 87 FeiFeiLi â€“ Stanford Univ. Prof. &amp; Chief Scientist of AI/ML of Google Cloud
<img alt="ì˜ìƒ" src="https://velog.velcdn.com/images/mi_nini/post/a9dbf5bf-39ed-4193-8b43-3182d791c604/image.png" /></p>
<p><a href="https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures/transcript">ì˜ìƒ</a> -&gt; ì»´í“¨í„°ì— ì‹œê°ëŠ¥ë ¥ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒ í•´ì™”ë˜ ì—°êµ¬ ë°œí‘œ </p>
<h3 id="ğŸ“Œí•©ì„±ê³±--ì‹ ê²½ë§--cnn-convolutional-neural-network">ğŸ“Œí•©ì„±ê³±  ì‹ ê²½ë§  CNN Convolutional Neural Network</h3>
<p>â€¢ í•©ì„±ê³± ì‹ ê²½ë§(CNN)
â»   ëŒ€ë‡Œ ì‹œê° í”¼ì§ˆ ì—°êµ¬ì—ì„œ ì‹œì‘,  â€˜80ë…„ëŒ€ë¶€í„° ì´ë¯¸ì§€ ì¸ì‹ ë¶„ì•¼ì— ì‚¬ìš©</p>
<h3 id="ë°ì´ë¹„ë“œ-í—ˆë¸”ê³¼-í† ë¥´ìŠ¤í…-ë¹„ì…€-ì‹œê°-í”¼ì§ˆì˜-êµ¬ì¡°ì—-ëŒ€í•œ-ì—°êµ¬1958">ë°ì´ë¹„ë“œ í—ˆë¸”ê³¼ í† ë¥´ìŠ¤í… ë¹„ì…€, ì‹œê° í”¼ì§ˆì˜ êµ¬ì¡°ì— ëŒ€í•œ ì—°êµ¬(1958)</h3>
<p>â»   ì‹œê° í”¼ì§ˆ ì•ˆì˜ ë§ì€ ë‰´ëŸ°ì´ ì‘ì€ êµ­ë¶€ ìˆ˜ìš©ì¥ì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ë°œê²¬
â»   ë‰´ëŸ°ì˜ ìˆ˜ìš©ì¥ë“¤ì€ ì„œë¡œ ê²¹ì¹  ìˆ˜ ìˆì–´ì„œ, í•©ì¹˜ë©´ ì „ì²´ ì‹œì•¼ë¥¼ ê°ì‹¸ê²Œ ë¨
â»   ë˜í•œ ì–´ë–¤ ë‰´ëŸ°ì€ ìˆ˜í‰ì„ ì˜ ì´ë¯¸ì§€ì—ë§Œ ë°˜ì‘í•˜ê³  ë°˜ë©´ ë‹¤ë¥¸ ë‰´ëŸ°ì€ ë‹¤ë¥¸ ê°ë„ì˜ ì„ ë¶„ì— ë°˜ì‘ 
â»   ì–´ë–¤ ë‰´ëŸ°ì€ í° ìˆ˜ìš©ì¥ì„ ê°€ì ¸ì„œ ì €ìˆ˜ì¤€ íŒ¨í„´ì´ ì¡°í•©ëœ ë” ë³µì¡í•œ íŒ¨í„´ì— ë°˜ì‘
â†’ ê³ ìˆ˜ì¤€ ë‰´ëŸ°ì´ ì´ì›ƒí•œ ì €ìˆ˜ì¤€ ë‰´ëŸ°ì˜ ì¶œë ¥ì— ê¸°ë°˜í•œë‹¤ëŠ” ì•„ì´ë””ì–´ê°€ ë„ì¶œ
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/a7412bbe-2a5e-42d6-b953-00ea999a42f6/image.png" /></p>
<h3 id="ğŸ“Œê°€ì¥-ì´ˆê¸°ì˜-cnn-ëª¨ë¸">ğŸ“Œê°€ì¥ ì´ˆê¸°ì˜ CNN ëª¨ë¸</h3>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/7e621f64-c434-45b2-ab82-a312fde77dc3/image.png" />
â€¢ ì‹œê° í”¼ì§ˆì— ëŒ€í•œ ì´ëŸ° ì—°êµ¬ëŠ” 1980ë…„ ì‹ ì¸ì‹ê¸°ì— ì˜ê°ì„ ì£¼ì—ˆê³ , ì§€ê¸ˆ í•©ì„±ê³± 
ì‹ ê²½ë§ìœ¼ë¡œ ì§„í™”
â»   1998ë…„ ì–€ ë¥´ì¿¤ LeNet-5
â†’ í•©ì„±ê³± ì¸µ(convolution layer), í’€ë§ ì¸µ(pooling layer) ì†Œê°œ
2ì°¨ì› ë°ì´í„° ì´ë¯¸ì§€ë¥¼ ê·¸ëŒ€ë¡œ ë„£ìŒ -&gt; í•™ìŠµê²°ê³¼ê°€ ì¢‹ì„ ìˆ˜ ë°–ì— ì—†ë‹¤. </p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/aa48c789-eb78-46a4-9ddf-0c9de120612a/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/b875e5da-a9fd-4607-824f-bacba81e7a09/image.png" /></th>
</tr>
</thead>
</table>
<h3 id="ì°¸ê³ -cnn-ì²´í—˜í•˜ê¸°">[ì°¸ê³ ] CNN ì²´í—˜í•˜ê¸°</h3>
<p>â€¢ <a href="https://transcranial.github.io/keras-js/#/">https://transcranial.github.io/keras-js/#/</a>
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/79fc558e-f177-46f1-ac19-d7143586d4c3/image.png" /></p>
<h3 id="ğŸ“Œí•©ì„±ê³±--ì¸µ--convolution-layer">ğŸ“Œí•©ì„±ê³±  ì¸µ  Convolution Layer</h3>
<p>â€¢ CNNì—ì„œëŠ” í•©ì„±ê³± ì—°ì‚°ì„ ì´ìš©í•˜ì—¬ í•˜ìœ„ ë ˆì´ì–´ì˜ ë…¸ë“œë“¤ê³¼ ìƒìœ„ ë ˆì´ì–´ì˜ ë…¸ë“œ 
ë“¤ì„ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ì—°ê²°ì‹œí‚´
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/0ac6b23d-69fd-4469-9139-e4d3cb6bf780/image.PNG" /></p>
<h3 id="ğŸ“Œí•©ì„±ê³±--convolution">ğŸ“Œí•©ì„±ê³±  Convolution</h3>
<p>â€¢ í•©ì„±ê³±(Convolution)
â»   ì£¼ë³€ í™”ì†Œê°’ë“¤ì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ì„œ ë”í•œ í›„ì— ì´ê²ƒì„ ìƒˆë¡œìš´ í™”ì†Œê°’ìœ¼ë¡œ í•˜ëŠ” ì—°ì‚°
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/c7437df1-1716-46d6-ae1f-f8a7ab4568a0/image.png" /></p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/39ef4060-ba59-48ec-b0bf-2d574835f747/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/5e16e2fc-729f-4ed5-a8ca-a8fd0d6555d0/image.png" /></th>
</tr>
</thead>
</table>
<h3 id="ğŸ“ŒíŒ¨ë”©--padding">ğŸ“ŒíŒ¨ë”©  Padding</h3>
<p>â€¢ íŒ¨ë”©(Padding) = Pad+ing íŒ¨ë“œë¥¼ ê°€ì ¸ë‹¤ ëŒ€ëŠ” ê²ƒ
â»   ì…ë ¥ ì˜ìƒì˜ ë†’ì´ì™€ ë„ˆë¹„ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ ì…ë ¥ ì˜ìƒ ì£¼ë³€ì— íŠ¹ì • ê°’ì„ ì¶”ê°€í•˜ëŠ” ê²ƒ 
â»   íŠ¹íˆ ì˜ìƒ ì£¼ë³€ì— 0ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì„ ì œë¡œ íŒ¨ë”©(zero padding)ì´ë¼ í•¨</p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/e6c77481-d7af-48ad-abdc-7a9157fd75fc/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/6ae6696b-6f0e-4da9-a7af-0ebf81ae48b0/image.png" /></th>
</tr>
</thead>
</table>
<h3 id="ğŸ“ŒìŠ¤íŠ¸ë¼ì´ë“œ--stride">ğŸ“ŒìŠ¤íŠ¸ë¼ì´ë“œ  Stride</h3>
<p>â€¢ ìŠ¤íŠ¸ë¼ì´ë“œ(Stride)
â»   ìˆ˜ìš©ì¥ ì‚¬ì´ì— ê°„ê²©ì„ ë‘ì–´ í° ì…ë ¥ì¸µì„ í›¨ì”¬ ì‘ì€ ì¸µì— ì—°ê²°í•˜ëŠ” ê²ƒ 
â»   ì¦‰, í•©ì„±ê³± ì—°ì‚°ì„ ì¼ì • ê°„ê²©ì„ ë‘ê³  ì‹¤í–‰í•˜ëŠ” ê²ƒ
â»   íŠ¹ì§•ì˜ ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ì—­í• </p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/fcf9ee57-e489-4d20-99bf-00a9025b2a8b/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/e08ea135-fb77-478f-b252-fedbc65b7a62/image.png" /></th>
</tr>
</thead>
</table>
<h3 id="ğŸ“Œí•©ì„±ê³±ì˜--ì—­í• ">ğŸ“Œí•©ì„±ê³±ì˜  ì—­í• </h3>
<p>â€¢ ìˆ˜í‰ ì„±ë¶„ í•„í„°ì™€ ìˆ˜ì§ ì„±ë¶„ í•„í„° ì ìš© ê²°ê³¼ ì˜ˆì‹œ</p>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/317b0cfb-58be-4a84-99cc-db4f928c79b0/image.png" /></p>
<p>â€¢ ë‹¤ì–‘í•œ íŠ¹ì§•ì˜ í•„í„°ë¥¼ ì´ ìš©í•˜ì—¬ íŠ¹ì§•ë§µ(feature map)ì„ ìƒì„±
â€¢ CNNì€ ìë™ìœ¼ë¡œ ê°€ì¥ ìœ  ìš©í•œ í•„í„°ë¥¼ ì°¾ìŒ
â€¢ ìƒìœ„ì¸µì€ í•˜ìœ„ íŠ¹ì§•ë“¤ì„ ì—°ê²°í•˜ì—¬ ë” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ</p>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/f95f98e8-71a2-484e-8376-e4f08413d2e1/image.png" /></p>
<h3 id="ğŸ“Œí•©ì„±ê³±--ì¸µ--êµ¬í˜„--ë°©ë²•">ğŸ“Œí•©ì„±ê³±  ì¸µ  êµ¬í˜„  ë°©ë²•</h3>
<blockquote>
<h3 id="tensorflow">Tensorflow</h3>
<p>filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)
filters[:, 3, :, 0] = 1    # ìˆ˜ì§
filters[3, :, :, 1] = 1    # ìˆ˜í‰
output = tf.nn.conv2d(images, filters, strides=1, padding=â€œSAMEâ€) </p>
</blockquote>
<h2 id="keras">Keras</h2>
<p>output = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, 
padding=â€œsameâ€, activation=â€œreluâ€)
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/b6e91c4e-b143-443b-af75-b253a61cf195/image.png" /></p>
<h3 id="ğŸ“Œí’€ë§--ì¸µ--pooling-layer">ğŸ“Œí’€ë§  ì¸µ  Pooling Layer</h3>
<p>â€¢ í’€ë§ ì¸µ(pooling layer)
â»   ê³„ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì„œë¸Œ ìƒ˜í”Œë§ í•˜ëŠ” ê²ƒ 
â»   íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ë¥¼ ì¤„ì´ë©´ ê³¼ëŒ€ ì í•©ì˜ ìœ„í—˜ë„ ì¤„ì–´ë“¦
â»   í¬ê¸°, ìŠ¤íŠ¸ë¼ì´ë“œ, íŒ¨ë”© ìœ í˜• ì§€ì • í•„ìš” 
â»   í’€ë§ ì¸µì—ëŠ” ê°€ì¤‘ì¹˜ê°€ ì—†ìŒ
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/939581c8-fcbc-494f-b941-11f3bff7526c/image.png" /></p>
<h3 id="-ìµœëŒ€-í’€ë§max-pooling">â€¢ ìµœëŒ€ í’€ë§(max pooling)</h3>
<p>â»   ê° ìˆ˜ìš©ì¥ì—ì„œ ê°€ì¥ í° ì…ë ¥ê°’ì´ ë‹¤ìŒ ì¸µìœ¼ë¡œ ì „ë‹¬ë˜ê³  ë‚˜ë¨¸ì§€ ê°’ë“¤ì€ ë²„ë ¤ì§ 
â»   2X2 ìµœëŒ€ í’€ë§, ìŠ¤íŠ¸ë¼ì´ë“œ 2, íŒ¨ë”© X
(â€¢ íŠ¹ì§•ì´ 1/4ë¡œ ê°ì†Œ)
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/24430602-660b-4ea6-b5d4-66072cae700d/image.png" /></p>
<h3 id="ğŸ“Œí’€ë§ê³¼--ë¶ˆë³€ì„±">ğŸ“Œí’€ë§ê³¼  ë¶ˆë³€ì„±</h3>
<p>â€¢ ë¶ˆë³€ì„±(invariance) ì œê³µ 
â»   ì´ë™ ë¶ˆë³€ì„±
â»   íšŒì „ê³¼ í™•ëŒ€, ì¶•ì†Œì— ëŒ€í•œ ì•½ê°„ì˜ ë¶ˆë³€ì„±ë„ ì œê³µ</p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/a09365ed-84a3-4229-bf8d-afcd33791be8/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/f92403f6-c8f4-44d1-a30b-0799e096e483/image.png" /></th>
</tr>
</thead>
<tbody><tr>
<td>â€¢ ê¹Šì´ ë°©í–¥ ìµœëŒ€ í’€ë§ ì‚¬ìš©ì‹œ</td>
<td></td>
</tr>
<tr>
<td>â»   ëª¨ë“  ë¶ˆë³€ì„± í•™ìŠµ ê°€ëŠ¥</td>
<td></td>
</tr>
</tbody></table>
<h3 id="ğŸ“Œí’€ë§--ì¸µ--êµ¬í˜„--ë°©ë²•">ğŸ“Œí’€ë§  ì¸µ  êµ¬í˜„  ë°©ë²•</h3>
<blockquote>
<h2 id="ìµœëŒ€-í’€ë§">ìµœëŒ€ í’€ë§</h2>
</blockquote>
<h3 id="tensorflow-1">Tensorflow</h3>
<p>output = tf.nn.max_pool(images, ksize(2, 2, 1, 1), strides=(2, 2, 1, 1), padding=â€œvalidâ€) </p>
<h3 id="keras-1">Keras</h3>
<p>output = keras.layers.MaxPool2D(pool_size=2) </p>
<h3 id="keras-ì „ì—­-í‰ê· -í’€ë§">Keras ì „ì—­ í‰ê·  í’€ë§</h3>
<p>global_avg_pool = keras.layers.GlobalAvgPool2D() </p>
<h3 id="ê¹Šì´-ë°©í–¥-ìµœëŒ€-í’€ë§">ê¹Šì´ ë°©í–¥ ìµœëŒ€ í’€ë§</h3>
<h3 id="tensorflow-2">Tensorflow</h3>
<p>output = tf.nn.max_pool(images, ksize(1, 1, 1, 3), strides=(1, 1, 1, 3), padding=â€œvalidâ€) </p>
<h3 id="kerasëŠ”-ì§€ì›-x-lambda-ì¸µ--tensorflowì˜-max_poolì„-í™œìš©í•˜ì—¬-êµ¬í˜„">KerasëŠ” ì§€ì› X, Lambda ì¸µ + Tensorflowì˜ max_poolì„ í™œìš©í•˜ì—¬ êµ¬í˜„</h3>
<p>depth_pool = keras.layers.Lambda(
lamda X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding=â€œvalidâ€))</p>
<h3 id="ğŸ“Œcnn-êµ¬ì¡°">ğŸ“ŒCNN êµ¬ì¡°</h3>
<p>â€¢ ì „í˜•ì ì¸ CNN êµ¬ì¡°
â»   í•©ì„±ê³± ì¸µì„ ëª‡ ê°œ ìŒ“ê³ (ê°ê° ReLU ì¸µì„ ê·¸ ë’¤ì— ë†“ê³ ), ê·¸ ë‹¤ìŒì— í’€ë§ì¸µì„ ìŒ“ê³ , 
ë˜ í•©ì„±ê³± ì¸µ(+ReLU)ì„ ëª‡ ê°œ ë” ìŒ“ê³ , ê·¸ ë‹¤ìŒì— ë‹¤ì‹œ í’€ë§ ì¸µì„ ìŒ“ëŠ” ë°©ì‹
â»   ë„¤íŠ¸ì›Œí¬ë¥¼ í†µê³¼í•˜ì—¬ ì§„í–‰í• ìˆ˜ë¡ ì´ë¯¸ì§€ëŠ” ì ì  ì‘ì•„ì§€ì§€ë§Œ, í•©ì„±ê³± ì¸µ ë•Œë¬¸ì— ì¼ë°˜ì ìœ¼ë¡œ ì ì  ë” 
ê¹Šì–´ì§ â†’ ë” ë§ì€ íŠ¹ì§• ë§µì„ ê°€ì§€ê²Œ ë¨
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/8bf04fdb-36c7-4fdb-a033-32a601ece2bb/image.png" /></p>
<h3 id="ğŸ“Œmnist-ë°ì´í„°--ë¶„ë¥˜">ğŸ“ŒMNIST ë°ì´í„°  ë¶„ë¥˜</h3>
<p>â€¢ í…ì„œí”Œë¡œìš° íŠœí† ë¦¬ì–¼ì— ë‚˜ì˜¤ëŠ” íŒ¨ì…˜ MNIST ë°ì´í„°ì…‹
â€¢ DNNê³¼ CNNì„ ì´ìš©í•˜ì—¬ ë¶„ë¥˜ ë° ê²°ê³¼ ë¹„êµ
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/04df1088-70aa-42a3-8f03-54be8d8fbbc1/image.png" /></p>
<h3 id="ğŸ’»dnnì„--ì´ìš©í•œ--ì˜ìƒ--ë¶„ë¥˜">ğŸ’»DNNì„  ì´ìš©í•œ  ì˜ìƒ  ë¶„ë¥˜</h3>
<pre><code class="language-py">import tensorflow as tf
from tensorflow import keras 
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import datasets, layers, models 

fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() 

train_images = train_images / 255.0
test_images = test_images / 255.0

model = models.Sequential()
model.add(layers.Flatten(input_shape=(28, 28)))
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', 
metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=5)</code></pre>
<h3 id="ğŸ’»cnnì„--ì´ìš©í•œ--ì˜ìƒ--ë¶„ë¥˜">ğŸ’»CNNì„  ì´ìš©í•œ  ì˜ìƒ  ë¶„ë¥˜</h3>
<pre><code class="language-python">from tensorflow.keras import datasets, layers, models

(train_images, train_labels), (test_images, test_labels) = datasets.fashion_mnist.load_data() 
train_images = train_images / 255.0
test_images = test_images / 255.0

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracyâ€™])
model.summary()

model.fit(train_images, train_labels, epochs=5)

test_loss, test_acc = model.evaluate(test_images, test_labels)</code></pre>