<h3 id="📝목차">📝목차</h3>
<h3 id="합성곱-신경망-convolutional-neural-network">합성곱 신경망 Convolutional Neural Network</h3>
<p>⁻  CNN 아이디어와 기술
• 합성곱 층
• Padding과 Stride
• 풀링 층</p>
<p>⁻  CNN 구조</p>
<p>⁻  DNN과 CNN 비교</p>
<p>How we're teaching computers to understand pictures? 87 FeiFeiLi – Stanford Univ. Prof. &amp; Chief Scientist of AI/ML of Google Cloud
<img alt="영상" src="https://velog.velcdn.com/images/mi_nini/post/a9dbf5bf-39ed-4193-8b43-3182d791c604/image.png" /></p>
<p><a href="https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures/transcript">영상</a> -&gt; 컴퓨터에 시각능력을 부여하는 것 해왔던 연구 발표 </p>
<h3 id="📌합성곱--신경망--cnn-convolutional-neural-network">📌합성곱  신경망  CNN Convolutional Neural Network</h3>
<p>• 합성곱 신경망(CNN)
⁻   대뇌 시각 피질 연구에서 시작,  ‘80년대부터 이미지 인식 분야에 사용</p>
<h3 id="데이비드-허블과-토르스텐-비셀-시각-피질의-구조에-대한-연구1958">데이비드 허블과 토르스텐 비셀, 시각 피질의 구조에 대한 연구(1958)</h3>
<p>⁻   시각 피질 안의 많은 뉴런이 작은 국부 수용장을 가진다는 것을 발견
⁻   뉴런의 수용장들은 서로 겹칠 수 있어서, 합치면 전체 시야를 감싸게 됨
⁻   또한 어떤 뉴런은 수평선의 이미지에만 반응하고 반면 다른 뉴런은 다른 각도의 선분에 반응 
⁻   어떤 뉴런은 큰 수용장을 가져서 저수준 패턴이 조합된 더 복잡한 패턴에 반응
→ 고수준 뉴런이 이웃한 저수준 뉴런의 출력에 기반한다는 아이디어가 도출
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/a7412bbe-2a5e-42d6-b953-00ea999a42f6/image.png" /></p>
<h3 id="📌가장-초기의-cnn-모델">📌가장 초기의 CNN 모델</h3>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/7e621f64-c434-45b2-ab82-a312fde77dc3/image.png" />
• 시각 피질에 대한 이런 연구는 1980년 신인식기에 영감을 주었고, 지금 합성곱 
신경망으로 진화
⁻   1998년 얀 르쿤 LeNet-5
→ 합성곱 층(convolution layer), 풀링 층(pooling layer) 소개
2차원 데이터 이미지를 그대로 넣음 -&gt; 학습결과가 좋을 수 밖에 없다. </p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/aa48c789-eb78-46a4-9ddf-0c9de120612a/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/b875e5da-a9fd-4607-824f-bacba81e7a09/image.png" /></th>
</tr>
</thead>
</table>
<h3 id="참고-cnn-체험하기">[참고] CNN 체험하기</h3>
<p>• <a href="https://transcranial.github.io/keras-js/#/">https://transcranial.github.io/keras-js/#/</a>
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/79fc558e-f177-46f1-ac19-d7143586d4c3/image.png" /></p>
<h3 id="📌합성곱--층--convolution-layer">📌합성곱  층  Convolution Layer</h3>
<p>• CNN에서는 합성곱 연산을 이용하여 하위 레이어의 노드들과 상위 레이어의 노드 
들을 부분적으로만 연결시킴
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/0ac6b23d-69fd-4469-9139-e4d3cb6bf780/image.PNG" /></p>
<h3 id="📌합성곱--convolution">📌합성곱  Convolution</h3>
<p>• 합성곱(Convolution)
⁻   주변 화소값들에 가중치를 곱해서 더한 후에 이것을 새로운 화소값으로 하는 연산
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/c7437df1-1716-46d6-ae1f-f8a7ab4568a0/image.png" /></p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/39ef4060-ba59-48ec-b0bf-2d574835f747/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/5e16e2fc-729f-4ed5-a8ca-a8fd0d6555d0/image.png" /></th>
</tr>
</thead>
</table>
<h3 id="📌패딩--padding">📌패딩  Padding</h3>
<p>• 패딩(Padding) = Pad+ing 패드를 가져다 대는 것
⁻   입력 영상의 높이와 너비를 유지하기 위해 입력 영상 주변에 특정 값을 추가하는 것 
⁻   특히 영상 주변에 0을 추가하는 것을 제로 패딩(zero padding)이라 함</p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/e6c77481-d7af-48ad-abdc-7a9157fd75fc/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/6ae6696b-6f0e-4da9-a7af-0ebf81ae48b0/image.png" /></th>
</tr>
</thead>
</table>
<h3 id="📌스트라이드--stride">📌스트라이드  Stride</h3>
<p>• 스트라이드(Stride)
⁻   수용장 사이에 간격을 두어 큰 입력층을 훨씬 작은 층에 연결하는 것 
⁻   즉, 합성곱 연산을 일정 간격을 두고 실행하는 것
⁻   특징의 차원을 축소하는 역할</p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/fcf9ee57-e489-4d20-99bf-00a9025b2a8b/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/e08ea135-fb77-478f-b252-fedbc65b7a62/image.png" /></th>
</tr>
</thead>
</table>
<h3 id="📌합성곱의--역할">📌합성곱의  역할</h3>
<p>• 수평 성분 필터와 수직 성분 필터 적용 결과 예시</p>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/317b0cfb-58be-4a84-99cc-db4f928c79b0/image.png" /></p>
<p>• 다양한 특징의 필터를 이 용하여 특징맵(feature map)을 생성
• CNN은 자동으로 가장 유 용한 필터를 찾음
• 상위층은 하위 특징들을 연결하여 더 복잡한 패턴 학습</p>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/f95f98e8-71a2-484e-8376-e4f08413d2e1/image.png" /></p>
<h3 id="📌합성곱--층--구현--방법">📌합성곱  층  구현  방법</h3>
<blockquote>
<h3 id="tensorflow">Tensorflow</h3>
<p>filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)
filters[:, 3, :, 0] = 1    # 수직
filters[3, :, :, 1] = 1    # 수평
output = tf.nn.conv2d(images, filters, strides=1, padding=“SAME”) </p>
</blockquote>
<h2 id="keras">Keras</h2>
<p>output = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, 
padding=“same”, activation=“relu”)
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/b6e91c4e-b143-443b-af75-b253a61cf195/image.png" /></p>
<h3 id="📌풀링--층--pooling-layer">📌풀링  층  Pooling Layer</h3>
<p>• 풀링 층(pooling layer)
⁻   계산량과 메모리 사용량, 파라미터 수를 줄이기 위해 입력 이미지를 서브 샘플링 하는 것 
⁻   파라미터의 수를 줄이면 과대 적합의 위험도 줄어듦
⁻   크기, 스트라이드, 패딩 유형 지정 필요 
⁻   풀링 층에는 가중치가 없음
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/939581c8-fcbc-494f-b941-11f3bff7526c/image.png" /></p>
<h3 id="-최대-풀링max-pooling">• 최대 풀링(max pooling)</h3>
<p>⁻   각 수용장에서 가장 큰 입력값이 다음 층으로 전달되고 나머지 값들은 버려짐 
⁻   2X2 최대 풀링, 스트라이드 2, 패딩 X
(• 특징이 1/4로 감소)
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/24430602-660b-4ea6-b5d4-66072cae700d/image.png" /></p>
<h3 id="📌풀링과--불변성">📌풀링과  불변성</h3>
<p>• 불변성(invariance) 제공 
⁻   이동 불변성
⁻   회전과 확대, 축소에 대한 약간의 불변성도 제공</p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/a09365ed-84a3-4229-bf8d-afcd33791be8/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/f92403f6-c8f4-44d1-a30b-0799e096e483/image.png" /></th>
</tr>
</thead>
<tbody><tr>
<td>• 깊이 방향 최대 풀링 사용시</td>
<td></td>
</tr>
<tr>
<td>⁻   모든 불변성 학습 가능</td>
<td></td>
</tr>
</tbody></table>
<h3 id="📌풀링--층--구현--방법">📌풀링  층  구현  방법</h3>
<blockquote>
<h2 id="최대-풀링">최대 풀링</h2>
</blockquote>
<h3 id="tensorflow-1">Tensorflow</h3>
<p>output = tf.nn.max_pool(images, ksize(2, 2, 1, 1), strides=(2, 2, 1, 1), padding=“valid”) </p>
<h3 id="keras-1">Keras</h3>
<p>output = keras.layers.MaxPool2D(pool_size=2) </p>
<h3 id="keras-전역-평균-풀링">Keras 전역 평균 풀링</h3>
<p>global_avg_pool = keras.layers.GlobalAvgPool2D() </p>
<h3 id="깊이-방향-최대-풀링">깊이 방향 최대 풀링</h3>
<h3 id="tensorflow-2">Tensorflow</h3>
<p>output = tf.nn.max_pool(images, ksize(1, 1, 1, 3), strides=(1, 1, 1, 3), padding=“valid”) </p>
<h3 id="keras는-지원-x-lambda-층--tensorflow의-max_pool을-활용하여-구현">Keras는 지원 X, Lambda 층 + Tensorflow의 max_pool을 활용하여 구현</h3>
<p>depth_pool = keras.layers.Lambda(
lamda X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding=“valid”))</p>
<h3 id="📌cnn-구조">📌CNN 구조</h3>
<p>• 전형적인 CNN 구조
⁻   합성곱 층을 몇 개 쌓고(각각 ReLU 층을 그 뒤에 놓고), 그 다음에 풀링층을 쌓고, 
또 합성곱 층(+ReLU)을 몇 개 더 쌓고, 그 다음에 다시 풀링 층을 쌓는 방식
⁻   네트워크를 통과하여 진행할수록 이미지는 점점 작아지지만, 합성곱 층 때문에 일반적으로 점점 더 
깊어짐 → 더 많은 특징 맵을 가지게 됨
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/8bf04fdb-36c7-4fdb-a033-32a601ece2bb/image.png" /></p>
<h3 id="📌mnist-데이터--분류">📌MNIST 데이터  분류</h3>
<p>• 텐서플로우 튜토리얼에 나오는 패션 MNIST 데이터셋
• DNN과 CNN을 이용하여 분류 및 결과 비교
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/04df1088-70aa-42a3-8f03-54be8d8fbbc1/image.png" /></p>
<h3 id="💻dnn을--이용한--영상--분류">💻DNN을  이용한  영상  분류</h3>
<pre><code class="language-py">import tensorflow as tf
from tensorflow import keras 
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import datasets, layers, models 

fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() 

train_images = train_images / 255.0
test_images = test_images / 255.0

model = models.Sequential()
model.add(layers.Flatten(input_shape=(28, 28)))
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', 
metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=5)</code></pre>
<h3 id="💻cnn을--이용한--영상--분류">💻CNN을  이용한  영상  분류</h3>
<pre><code class="language-python">from tensorflow.keras import datasets, layers, models

(train_images, train_labels), (test_images, test_labels) = datasets.fashion_mnist.load_data() 
train_images = train_images / 255.0
test_images = test_images / 255.0

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy’])
model.summary()

model.fit(train_images, train_labels, epochs=5)

test_loss, test_acc = model.evaluate(test_images, test_labels)</code></pre>