<blockquote>
<h2 id="ğŸ“ëª©ì°¨">ğŸ“ëª©ì°¨</h2>
<p>T- he Meaning of Tensors in RNN</p>
</blockquote>
<ul>
<li>Attention Mechanisms</li>
<li>Sequence to Sequence</li>
<li>Transformer<ul>
<li>Attention is all you need!</li>
</ul>
</li>
</ul>
<hr />
<h2 id="ğŸ“Œspan-stylecolorindianredë³´ë„ˆìŠ¤---embedding-layerspan">ğŸ“Œ<span style="color: indianred;">ë³´ë„ˆìŠ¤ - Embedding layer</span></h2>
<h3 id="embedding-in-nn">Embedding in NN</h3>
<ul>
<li>NLP ì²« ë ˆì´ì–´ë¡œ í™œìš©
Lookup Tableë¡œ êµ¬ì„±
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/ad081114-e926-446e-b6c5-764f03bbe669/image.png" />
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/b6241ff4-8972-42f3-a459-453c03937c37/image.png" /></li>
</ul>
<blockquote>
<h2 id="ê³µì‹-ë¬¸ì„œ-ì°¸ê³ ">ê³µì‹ ë¬¸ì„œ ì°¸ê³ </h2>
<p>Official Docs
PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html">https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html</a> 
TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding</a> </p>
</blockquote>
<hr />
<h2 id="ğŸ“Œspan-stylecolorindianredrnn-tensors-ì˜ë¯¸span">ğŸ“Œ<span style="color: indianred;">RNN Tensors ì˜ë¯¸</span></h2>
<h2 id="í…ì„œ-tensor-ì˜ë¯¸">í…ì„œ (Tensor) ì˜ë¯¸</h2>
<p>ì‚¬ì „ì  ì˜ë¯¸ (ì˜¨ë¼ì¸ ìœ„í‚¤, <a href="https://ko.wikipedia.org/wiki/%ED%85%90%EC%84%9C">https://ko.wikipedia.org/wiki/í…ì„œ</a>)
19ì„¸ê¸°ì— ì¹´ë¥¼ í”„ë¦¬ë“œë¦¬íˆ ê°€ìš°ìŠ¤ê°€ ê³¡ë©´ì— ëŒ€í•œ ë¯¸ë¶„ ê¸°í•˜í•™ì„ ë§Œë“¤ë©´ì„œ ë„ì…í•˜ì˜€ë‹¤. ê¸°ë³¸ì ì¸ ì˜ˆëŠ” ë‚´ì ê³¼ ì„ í˜• ë³€í™˜ì´ ìˆìœ¼ë©° ë¯¸ë¶„ ê¸°í•˜í•™ì—ì„œ ìì£¼ ë“±ì¥í•œë‹¤. í…ì„œëŠ” ê¸°ì €ë¥¼ ì„ íƒí•˜ì—¬ ë‹¤ì°¨ì› ë°°ì—´ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ê¸°ì €ë¥¼ ë°”ê¾¸ëŠ” ë³€í™˜ ë²•ì¹™ì´ ì¡´ì¬í•œë‹¤. </p>
<ul>
<li>ì‰½ê²Œ, 3ì°¨ì› ë²¡í„°ë¥¼ í…ì„œë¡œ ìƒê°í•˜ë©´ í¸ë¦¬í•˜ë‹¤.
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/7c92c6f4-f315-4b4d-b021-329a4424b3e7/image.png" /><blockquote>
<p>ì°¸ê³  
ë¸”ë¡œê·¸: <a href="https://rekt77.tistory.com/102">https://rekt77.tistory.com/102</a> 
ìœ íŠœë¸Œ: <a href="https://www.youtube.com/watch?v=m0qwxNA7IzI">https://www.youtube.com/watch?v=m0qwxNA7IzI</a> </p>
</blockquote>
</li>
</ul>
<ul>
<li>ì´ë¯¸ì§€(Image)ì—ì„œì˜ í…ì„œ
í‘ë°± ì´ë¯¸ì§€ -&gt; 3ì°¨ì› í…ì„œ
ì»¬ëŸ¬ ì´ë¯¸ì§€ -&gt; 4ì°¨ì› í…ì„œ</li>
</ul>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/0b930594-5474-4a15-a4d1-5765ed0151e7/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/9ae02824-6951-49e3-b546-d9ae1709348c/image.png" /></th>
</tr>
</thead>
</table>
<ul>
<li>ë™ì˜ìƒ(Video)ì—ì„œì˜ í…ì„œ
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/f4ff36a4-6be0-4b73-86a4-5f9ed5dffb63/image.png" /></li>
</ul>
<hr />
<h2 id="tensor-in-nlp">Tensor in NLP</h2>
<p>ì…ì¶œë ¥ í…ì„œ in Fully Connected Layer</p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/6ac0a352-3f7b-44f5-ab3c-41125132e4d6/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/cedbef95-4f2c-46f0-90e0-6010d478325e/image.png" /></th>
</tr>
</thead>
</table>
<hr />
<h2 id="tensor-in-nlp-1">Tensor in NLP</h2>
<ul>
<li>ì•ìª½ Padding í•˜ëŠ” ê²½ìš°</li>
</ul>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/119bf942-57f3-49aa-8f4e-c728739715ac/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/f833cd79-1801-4a9e-b600-781804b9ce42/image.png" /></th>
</tr>
</thead>
</table>
<ul>
<li>ë’¤ìª½ Padding í•˜ëŠ” ê²½ìš°</li>
</ul>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/502e09b9-da62-4267-b33c-7442039b73b1/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/085b0635-d068-4a37-bac5-09f492fc62f6/image.png" /></th>
</tr>
</thead>
</table>
<hr />
<h2 id="ğŸ“Œspan-stylecolorindianredattention-mechanismspan">ğŸ“Œ<span style="color: indianred;">Attention Mechanism</span></h2>
<h2 id="tokenizer-ë°-embedding-ê°œë…">Tokenizer ë° Embedding ê°œë…</h2>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/b596f2d8-41c7-4413-b5af-22938d71e4db/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/ac48a2f0-5f72-4a06-9ce8-f9d7f27e4a84/image.png" /></th>
</tr>
</thead>
<tbody><tr>
<td>í•œê¸€ì˜ ê²½ìš° í˜•íƒœì†Œë¶„ì„ ê³¼ì • ì¶”ê°€</td>
<td></td>
</tr>
<tr>
<td>Okt.morphs(phrase, norm=False, stem=False)</td>
<td></td>
</tr>
<tr>
<td>Parse phrase to morphemes.</td>
<td></td>
</tr>
</tbody></table>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/2efc1386-92fc-4f01-b72c-b8c6debce204/image.png" /></p>
<h2 id="ğŸ’»íŒŒì´ì¬-ì‚¬ì „dictionary-êµ¬ì¡°">ğŸ’»íŒŒì´ì¬ ì‚¬ì „(Dictionary) êµ¬ì¡°</h2>
<pre><code class="language-py">&gt;&gt;&gt; score = {}
&gt;&gt;&gt; score['sonic'] = 80
&gt;&gt;&gt; score['dante'] = 70
&gt;&gt;&gt; score['kim'] = 99
&gt;&gt;&gt; print(score['kim'])
99</code></pre>
<p>Query: ì§ˆì˜, ì°¾ê³ ì í•˜ëŠ” ëŒ€ìƒ (ë‚´ê°€ ì°¾ê³ ì í•˜ëŠ” ê²ƒì„ í¬í•¨í•˜ëŠ” ì§ˆë¬¸)
Key: ì°¾ì„ ê°’ì„ ì°¸ì¡°í•˜ëŠ” ê°’
Value: ì €ì¥ëœ ë°ì´í„° (Queryë¥¼ í†µí•´ ì°¾ì„ ê°’)
Dictionary: Python ìë£Œêµ¬ì¡° (key: value ìŒìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì§‘í•©)</p>
<hr />
<h2 id="attention-mechanismattention-score">Attention MechanismAttention Score</h2>
<p>Softmaxë¥¼ í†µí•´ Attention Distribution ê³„ì‚°</p>
<p>ê° ì¸ì½”ë”ì˜ Featureì™€ Attention ê°€ì¤‘ì¹˜ë¥¼ ê°€ì¤‘í•©(weighted sum)í•˜ì—¬ Attention Value ê³„ì‚°
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/ac8842e7-3ab6-46f7-bacf-b3a4151c0d48/image.png" /></p>
<ul>
<li>Queryì— ëŒ€í•˜ì—¬ ì–´ë–¤ Keyê°€ ìœ ì‚¬í•œì§€ë¥¼ ì°¾ëŠ”ë‹¤</li>
<li>Queryì™€ Keyì˜ ìœ ì‚¬ë„ ê°’ì„ ë°˜ì˜í•˜ì—¬ Value ê°’ë“¤ì„ í•©ì„±í•œë‹¤.
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/d683188c-4d0b-4e72-97e6-7cac92089904/image.png" /></li>
</ul>
<p>ì‚¬ì „ì—ì„œ ì§ˆì˜(Query)ì— ëŒ€í•œ ê°’ì„ ì°¾ëŠ” ê³¼ì •</p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/8751a844-11e8-4204-9175-5bf15a4de088/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/ed5e2faa-08fd-4bc7-a631-72a251ccb493/image.png" /></th>
</tr>
</thead>
</table>
<h2 id="attention-mechanism">Attention Mechanism</h2>
<ul>
<li>Attention Score êµ¬í•˜ê¸°
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/a96f1e2d-84a4-4172-980b-925677f1c52f/image.png" /></li>
</ul>
<h2 id="attention-mechanism-1">Attention Mechanism</h2>
<ul>
<li>Attention Score</li>
<li>Softmaxë¥¼ í†µí•´ Attention Distribution ê³„ì‚°
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/5db93c4d-8b7f-4985-b328-51a99d65badc/image.png" /></li>
</ul>
<h2 id="attention-score">Attention Score</h2>
<ul>
<li><p>Softmaxë¥¼ í†µí•´ Attention Distribution ê³„ì‚°</p>
</li>
<li><p>ê° ì¸ì½”ë”ì˜ Featureì™€ Attention ê°€ì¤‘ì¹˜ë¥¼ ê°€ì¤‘í•©(weighted sum)í•˜ì—¬ Attention Value ê³„ì‚°
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/8c20aea1-ce2c-4018-83d9-1998945ba1ba/image.png" /></p>
</li>
</ul>
<h2 id="attention-score-1">Attention Score</h2>
<ul>
<li><p>Softmaxë¥¼ í†µí•´ Attention Distribution ê³„ì‚°</p>
</li>
<li><p>ê° ì¸ì½”ë”ì˜ Featureì™€ Attention ê°€ì¤‘ì¹˜ë¥¼ ê°€ì¤‘í•©(weighted sum)í•˜ì—¬ Attention Value ê³„ì‚°</p>
</li>
<li><p>Attention Valueë¥¼ í˜„ì¬ì‹œì (t)ì— ì—°ê²°(concatenate)
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/413f469f-dee3-43d3-8f48-c8daa40bbfb0/image.png" /></p>
</li>
</ul>
<h2 id="attention-score-2">Attention Score</h2>
<ul>
<li><p>Softmaxë¥¼ í†µí•´ Attention Distribution ê³„ì‚°</p>
</li>
<li><p>ê° ì¸ì½”ë”ì˜ Featureì™€ Attention ê°€ì¤‘ì¹˜ë¥¼ ê°€ì¤‘í•©(weighted sum)í•˜ì—¬ Attention Value ê³„ì‚°</p>
</li>
<li><p>Attention Valueë¥¼ í˜„ì¬ì‹œì (t)ì— ì—°ê²°(concatenate)</p>
</li>
</ul>
<p><strong>- ì¶œë ¥ì¸µ ì—°ì‚°ì˜ ì…ë ¥ì´ ë˜ëŠ” ê°’ ê³„ì‚°</strong>
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/50dd9131-51e9-4b2e-abd2-2529de0647c2/image.png" /></p>
<hr />
<h2 id="ğŸ“Œspan-stylecolorindianredsequence-to-sequenceseq2seqspan">ğŸ“Œ<span style="color: indianred;">Sequence to Sequence(seq2seq)</span></h2>
<p>Sequence to Sequence (Seq2Sec)
ì¸ê³µì§€ëŠ¥ì„ í™œìš©í•œ ë²ˆì—­ ëª¨ë¸ì—ì„œ ë§ì´ ì‚¬ìš©</p>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/5590e43e-15b0-4f5f-87bb-069313e4cb5f/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/3341440a-0136-4a24-9789-3e605bbfbd1e/image.png" /></th>
</tr>
</thead>
<tbody><tr>
<td>- ëŒ€ë¶€ë¶„ì˜ Attention Network: Key, Valueë¥¼ ë™ì¼í•˜ê²Œ ì‚¬ìš©</td>
<td></td>
</tr>
<tr>
<td>- Seq2Sec: Encoderì˜ Hidden Layerë“¤ì„ key, valueë¡œ ì‚¬ìš©</td>
<td></td>
</tr>
</tbody></table>
<h2 id="seq2sec-key-value-ìƒì„±">Seq2Sec: Key-Value ìƒì„±</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/67991d55-e15f-4549-8dda-c67f51f416aa/image.png" /></p>
<h2 id="seq2sec-query">Seq2Sec: Query</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/17aaccb3-e449-4895-8c93-77f3c75850e1/image.png" /></p>
<h2 id="attention-value-ê³„ì‚°">Attention Value ê³„ì‚°</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/80bf8bcc-c5d8-44d8-92a1-a6739b74aa19/image.png" /></p>
<blockquote>
<h2 id="attention-ì‹ ê²½ë§_seq2seq-ì‹¤ìŠµ">Attention ì‹ ê²½ë§_seq2seq ì‹¤ìŠµ</h2>
<p><a href="https://github.com/bentrevett/pytorch-seq2seq">https://github.com/bentrevett/pytorch-seq2seq</a> </p>
</blockquote>
<hr />
<h2 id="ğŸ“Œspan-stylecolorindianredtransformerspan">ğŸ“Œ<span style="color: indianred;">Transformer</span></h2>
<h2 id="2016ë…„-êµ¬ê¸€ì€-gnmt-google-neural-machine-translation-ì‹œìŠ¤í…œ-ë°œí‘œ">2016ë…„ êµ¬ê¸€ì€ GNMT (Google Neural Machine Translation) ì‹œìŠ¤í…œ ë°œí‘œ</h2>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/e3c98ed6-259b-4eeb-9740-4445c9263306/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/5c88806c-6b0d-42b8-b1ac-24b3a5b23652/image.png" /></th>
</tr>
</thead>
<tbody><tr>
<td>Seq2Seq + Attention + ê°•í™”í•™ìŠµ(Reinforce Learning)</td>
<td></td>
</tr>
<tr>
<td>&gt; ë…¼ë¬¸ ë§í¬: <a href="https://arxiv.org/pdf/1609.08144.pdf">https://arxiv.org/pdf/1609.08144.pdf</a></td>
<td></td>
</tr>
</tbody></table>
<h2 id="ê¸°ê³„ë²ˆì—­-ì„±ëŠ¥í‰ê°€metric--ppl">ê¸°ê³„ë²ˆì—­ ì„±ëŠ¥í‰ê°€(metric) â€“ PPL</h2>
<p>PPL (Perplexity): 
í™•ë¥ ì˜ ì—­ìˆ˜, í—·ê°ˆë¦¬ëŠ” ì •ë„ 
ì˜ˆì¸¡ í™•ë¥ ì´ ë†’ìœ¼ë©´ ì¢‹ìŒ ë”°ë¼ì„œ ïƒ  PPL ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸
ê° time-stepë³„ ì‹¤ì œ ì •ë‹µì— ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ ë§Œ ì±„ì 
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/60a4d791-63ab-4781-8f9c-0674af46da38/image.png" /></p>
<p>ë²ˆì—­1ì´ ì˜ëœ ë²ˆì—­ì´ì§€ë§Œ, ì˜¤íˆë ¤ ë§ì¶˜ ê°œìˆ˜ê°€ ë§ì•„ í™•ë¥ ì´ Hit í™•ë¥ ì´ ë†’ì€ ë²ˆì—­2ê°€ PPLì´ ë‚®ë‹¤.
ë²ˆì—­ í’ˆì§ˆ í‰ê°€ metricìœ¼ë¡œ ì‚¬ìš©í•˜ê¸°ì—ëŠ” í•œê³„ì </p>
<h2 id="ê¸°ê³„ë²ˆì—­-ì„±ëŠ¥í‰ê°€metric--bleu">ê¸°ê³„ë²ˆì—­ ì„±ëŠ¥í‰ê°€(metric) â€“ BLEU</h2>
<p>BLEU (Bi-Lingual Evaluation Understudy)
ì •ë‹µê³¼ ì˜ˆì¸¡ ë¬¸ì¥ ì‚¬ì´ì—ì„œ ì¼ì¹˜í•˜ëŠ” n-gram ë³„ precisionì˜ ê°€ì¤‘(ê¸°í•˜) í‰ê·  ïƒ  ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
n-gramë³„ ì •ë°€ë„ì˜ í‰ê· ì„ ë°±ë¶„ë¥ ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒ
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/2e4becf8-b0c5-4418-bcd9-7486bb11fc4e/image.png" /></p>
<hr />
<h2 id="2017ë…„-í˜ì´ìŠ¤ë¶-fully-convolutional-seq2seq-ë°œí‘œ">2017ë…„ í˜ì´ìŠ¤ë¶, Fully Convolutional Seq2Seq ë°œí‘œ</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/2629d898-0f3d-49fa-b9fc-411dd6863598/image.png" /></p>
<blockquote>
<p>ë…¼ë¬¸ ë§í¬: <a href="https://arxiv.org/pdf/1705.03122.pdf">https://arxiv.org/pdf/1705.03122.pdf</a> </p>
</blockquote>
<h2 id="seq2seqì™€-ë¹„ìŠ·í•œ-êµ¬ì¡°-attention-ê¸°ìˆ ë§Œ-ì ìš©">Seq2Seqì™€ ë¹„ìŠ·í•œ êµ¬ì¡°, Attention ê¸°ìˆ ë§Œ ì ìš©</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/5b00adeb-ce59-4e7d-ad4a-d5731bd6956f/image.png" /></p>
<blockquote>
<p>ë…¼ë¬¸ ë§í¬: <a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a> </p>
</blockquote>
<hr />
<h2 id="multi-head-attenttion">Multi-head Attenttion</h2>
<p>Multi-head Attenttion
ì¢‹ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•œ ê²ƒ --&gt; ìµœì ì˜ ì§ˆì˜ (Query)ë¥¼ ë§Œë“œëŠ” ê²ƒ
ì¢‹ì€ Query ë§Œë“œëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ëŠ” ê²ƒ --&gt; Attention í•™ìŠµ
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/fb750ab9-bdae-4b6e-a767-9106c2325aef/image.png" /></p>
<p>ë§Œì•½ ë‹¤ì–‘í•˜ê²Œ í•  ìˆ˜ ìˆë‹¤ë©´ ì¢€ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì„ ê²ƒ
Attentionì„ ë™ì‹œì— ì—¬ëŸ¬ê°œ ìƒì„±í•˜ì—¬ ì¢…í•© --&gt; Multi-Head Attention</p>
<h3 id="transformer">Transformer</h3>
<ul>
<li>Attention is all you need! -&gt; ë…¼ë¬¸ì œëª© </li>
<li>CNN, RNN ì‚¬ìš©ì„ ë°°ì œí•˜ê³ , ì˜¤ì§ Attention Mechanismë§Œì„ ì‚¬ìš©í•˜ì—¬ ìµœê³ ì˜ ì„±ëŠ¥</li>
<li>í˜„ì¬ SOTA ê¸°ìˆ ì— ì†í•¨ (State â€“ of â€“ the - Art)<blockquote>
<p>ë…¼ë¬¸ ë§í¬: <a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a> </p>
</blockquote>
</li>
</ul>
<p>ëª¨ë¸ íŠ¹ì§•</p>
<ul>
<li>Scaled Dot-product Attention</li>
<li>Multi-head Attentionì´ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜</li>
<li>RNNì˜ BPTT ì—†ìŒ ïƒ  ë³‘ë ¬ê³„ì‚° ê°€ëŠ¥</li>
<li>Positional Encoding (ì…ë ¥ ë‹¨ì–´ ìœ„ì¹˜ í‘œì‹œ): ë³‘ë ¬ì—°ì‚° ì§€ì›
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/19d1fba1-a1fa-44b9-b29c-4b4f7eb1b4bf/image.png" /></li>
</ul>
<hr />
<h2 id="ğŸ“Œì „ì²´ì -êµ¬ì¡°_transformer">ğŸ“Œì „ì²´ì  êµ¬ì¡°_Transformer</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/8747c825-2d3c-454a-81a2-f3f7a8aac04d/image.png" /></p>
<blockquote>
<p>ë¹¨ê°•ìƒ‰: ì¸ì½”ë” (ì…ë ¥, Input)
íŒŒë‘ìƒ‰: ë””ì½”ë” (ì¶œë ¥, Output)
ì£¼í™©ìƒ‰: ì¸ì½”ë”ì—ì„œ Self-Attentionì´ ì¼ì–´ë‚˜ëŠ” ë¶€ë¶„
í•˜ëŠ˜ìƒ‰: ë””ì½”ë”ì—ì„œ Self-Attentionì´ ì¼ì–´ë‚˜ëŠ” ë¶€ë¶„
ë…¸ë€ìƒ‰: ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ Attentionì´ ì¼ì–´ë‚˜ëŠ” ë¶€ë¶„</p>
</blockquote>
<h2 id="ì…ì¶œë ¥-êµ¬ì¡°">ì…ì¶œë ¥ êµ¬ì¡°</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/6421e10c-235b-48f1-bb64-c7d1e75c0901/image.png" /></p>
<h2 id="word-embedding">Word Embedding</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/af5c44c6-6fb8-47fa-88d9-86a79c2726ed/image.png" /></p>
<h2 id="positional-encoding">Positional Encoding</h2>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/643bb2c9-bd65-46a5-b71c-2506e821ff3c/image.png" /></p>
<p>ì˜ˆ) â€œI love you but not love him.â€</p>
<p>ì•ì˜ loveì™€ ë’¤ì˜ loveì€ ì¼ë°˜ì ì¸ ì„ë² ë”©ë§Œì„ ê±°ì³¤ì„ ë•Œ ë™ì¼í•œ ê°’ì„ ê°€ì§</p>
<p>Positional Encoding ì´ë¼ëŠ” ì£¼ê¸°í•¨ìˆ˜ì— ì˜í•œ ìœ„ì¹˜ì— ë”°ë¥¸ ë‹¤ë¥¸ ì„ë² ë”©ì„ ê±°ì¹˜ë©´ ê°™ì€ ë‹¨ì–´ì—¬ë„ ë¬¸ì¥ì—ì„œ ì“°ì¸ ìœ„ì¹˜ì— ë”°ë¼ ë‹¤ë¥¸ ì„ë² ë”© ê°’ì„ ë¨.</p>
<h2 id="scaled-dot-product-attention">Scaled Dot-product Attention</h2>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/77502ebb-5c35-44a6-af46-7a6919660fcc/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/cd1d37cf-366d-40a3-b769-72458990eaa5/image.png" /></th>
</tr>
</thead>
</table>
<h2 id="multi-head-attention">Multi-head Attention</h2>
<table>
<thead>
<tr>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/de598c13-8201-4f13-972f-3cf35cc85b87/image.png" /></th>
<th><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/50351dcc-66ae-4336-9185-9835a7b63fb1/image.png" /></th>
</tr>
</thead>
<tbody><tr>
<td>- Linear ì—°ì‚°(Matrix Mult) ì´ìš©í•´ Q, K, V ì°¨ì› ê°ì†Œ</td>
<td></td>
</tr>
<tr>
<td>- hê°œì˜ Attention Layerë¥¼ ë³‘ë ¬ ê³„ì‚°</td>
<td></td>
</tr>
<tr>
<td>- ê³„ì‚°ëœ hê°œë¥¼ Concatenate</td>
<td></td>
</tr>
<tr>
<td>- í•„ìš”ì— ë”°ë¼ Linear ì—°ì‚°ì„ í†µí•´ ì°¨ì›ì„ ë³€ê²½</td>
<td></td>
</tr>
<tr>
<td>- ë³‘ë ¬ì²˜ë¦¬ì— ë§¤ìš° ìœ ë¦¬í•œ êµ¬ì¡°</td>
<td></td>
</tr>
</tbody></table>
<p>Attention ì‹ ê²½ë§_Transformer ì‹¤ìŠµ</p>
<blockquote>
<p>Github Transformer êµ¬í˜„
    - ë…ì¼ì–´-ì˜ì–´ ë²ˆì—­: <a href="https://github.com/hyunwoongko/transformer">https://github.com/hyunwoongko/transformer</a><br />    - ì˜ì–´-í•œêµ­ì–´ ë²ˆì—­: <a href="https://github.com/nawnoes/pytorch-transformer">https://github.com/nawnoes/pytorch-transformer</a> 
Transformer ì°¸ê³  ë¸”ë¡œê·¸<a href="https://rubikscode.net/2019/08/05/transformer-with-python-and-tensorflow-2-0-attention-layers/">https://rubikscode.net/2019/08/05/transformer-with-python-and-tensorflow-2-0-attention-layers/</a> </p>
</blockquote>