<h1 id="다층-퍼셉트론과-심층-신경망">다층 퍼셉트론과 심층 신경망</h1>
<h3 id="목차">목차</h3>
<h3 id="--다층-퍼셉트론-multi-layer-perceptron">•  다층 퍼셉트론 Multi-Layer Perceptron</h3>
<p>⁻   다층 퍼셉트론의 구조
⁻   은닉층
⁻   활성화 함수
⁻   오차 역전파 알고리즘
• 다층 퍼셉트론 구현
⁻   딥러닝 프레임워크
⁻   Tensorflow를 이용한 MLP 구현</p>
<h3 id="-심층-신경망-deep-neural-network">• 심층 신경망 Deep Neural Network</h3>
<p>⁻   딥러닝 성공 요인</p>
<blockquote>
<h2 id="들어가기전에">들어가기전에..</h2>
</blockquote>
<h4 id="input-layer입력층--입력값raw-ddata--가공되지-않은-측정-자료을-받습니다">Input layer(입력층) : 입력값(Raw ddata : 가공되지 않은 측정 자료)을 받습니다.</h4>
<h4 id="hidden-layer은닉층--데이터를-처리하고-신경망ㅇ이-수행하도록-설계된-작업을-합니다">Hidden layer(은닉층) : 데이터를 처리하고 신경망ㅇ이 수행하도록 설계된 작업을 합니다.</h4>
<h4 id="output-layer출력층--처리된-데이터를-기반으로-예측-또는-결정을-내립니다">Output layer(출력층) : 처리된 데이터를 기반으로 예측 또는 결정을 내립니다.</h4>
<h3 id="1-다층--퍼셉트론--mlpmulti-layer-perceptron">1. 다층  퍼셉트론  MLP(Multi-Layer Perceptron)</h3>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/0f04b4bc-e514-416b-89b3-325af9c78720/image.png" /></p>
<p>• 입력층과 출력층 사이에 은닉층(hidden layer)을 가지고 있는 신경망 
⁻   퍼셉트론을 병렬과 순차 구조로 결합한 구조
⁻   완전 연결(fully-connected) 구조 
⁻   층 수 = 은닉층 수 + 출력층 1개</p>
<p>• 다층 퍼셉트론의 핵심 아이디어 
⁻   은닉층 사용
⁻   시그모이드 활성화 함수 사용 -&gt; 전에는 계단 함수였는데..
⁻   오차 역전파 학습 알고리즘 -&gt; 학습의 난이도 증가</p>
<h3 id="은닉층--hidden-layer">은닉층  Hidden Layer</h3>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/572f8f86-b749-46e0-bf30-b8c66941f8ac/image.png" /></p>
<p>• 은닉층은 특징 추출기
⁻ 은닉층은 특징 벡터를 분류에 더 유리한 새로운 특징 공간으로 변환 
⁻ 현대 기계 학습에서는 특징 학습이라feature learning 부름
(딥러닝은 더 많은 단계를 거쳐 특징 학습을 함</p>
<h3 id="은닉층의--구조는--어떻게">은닉층의  구조는  어떻게?</h3>
<p>• 호닉의 주장[Hornik1989]
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/39e906b0-79c8-4e66-a9ed-be247bceb9b1/image.png" /></p>
<p>⁻   은닉 노드를 무수히 많게 할 수 없으므로, 실질적으로는 복잡한 구조의 데이터에서는 성능 한계</p>
<h3 id="활성화--함수--activation-function">활성화  함수  Activation Function</h3>
<p>• 단층 퍼셉트론의 활성화 함수
⁻   “퍼셉트론에서는 활성화 함수로 계단함수를 이용한다.”
⁻   계단 함수(step function): 임계값을 경계로 출력이 바뀌는 함수
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/e4eff579-8a20-4bfc-b5c1-a69f6640440e/image.png" />
• 활성화 함수를 계단 함수에서 다른 함수로 변경하는 것이 신경망의 세계로 나아 가는 열쇠!</p>
<h3 id="시그모이드--함수와--계단--함수--비교">시그모이드  함수와  계단  함수  비교</h3>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/d3231561-652b-4115-9c1a-0f42040bf080/image.png" /></p>
<p>• 차이점</p>
<ol>
<li>시그모이드 함수는 부드러운 곡선이며 입력에 따 라 출력이 연속적으로 변화, 계단 함수는 0을 경계로 출력이 갑자기 바뀜</li>
<li>시그모이드 함수는 실수, 계단 함수는 0과 1 중 하나의 값만 반환</li>
</ol>
<p>• 공통점</p>
<ol>
<li>입력이 작을 때의 출력은 0에 가깝고, 입력이 커지 
면 출력이 1에 가까워짐</li>
<li>입력이 아무리 작거나 커도 출력은 0~1 사이 값</li>
<li>비선형 함수<h3 id="활성화--함수--activation-function-1">활성화  함수  Activation Function</h3>
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/33d7c7d1-5d9d-4e16-b5f2-aa8c8b472514/image.png" />
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/e8609c50-7e87-4fbc-8a7a-6f07ad5d4287/image.png" />
• 미분 가능하고 연속적인 함수를 사용
⁻   학습 알고리즘이 활성화 함수의 일차 미분 값을 사용하기 때문</li>
</ol>
<ul>
<li><h4 id="활성화-함수-및-장단점-정리">활성화 함수 및 장단점 정리</h4>
</li>
<li><a href="https://nittaku.tistory.com/267">https://nittaku.tistory.com/267</a></li>
<li><a href="https://gooopy.tistory.com/51?category=824281">https://gooopy.tistory.com/51?category=824281</a></li>
</ul>
<h3 id="활성화--함수--activation-function-2">활성화  함수  Activation Function</h3>
<p>• 신경망이 사용하는 다양한 활성화 함수
⁻   로지스틱 시그모이드와 하이퍼볼릭 탄젠트는 a가 커질수록 계단함수에 가까워짐 
⁻   모두 1차 도함수 계산이 빠름 (특히 ReLU는 비교 연산 한 번)
<img alt="" src="https://velog.velcdn.com/images/mi_nini/post/90412a02-7411-4088-837e-d912b33e79b5/image.png" />
⁻   주로 퍼셉트론은 계단함수, 다층 퍼셉트론은 로지스틱 시그모이드와 하이퍼볼릭 탄젠트, 
딥러닝은 ReLU를 사용</p>
<h3 id="다층--퍼셉트론의--동작">다층  퍼셉트론의  동작</h3>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/f8e79e69-a408-4393-b409-f0749d042e1b/image.png" />
• 순방향 전파(feed-forward)를 통해 출력층에서 계산되어 나오는 값을 사용 
⁻   각 노드에서의 출력 값은 퍼셉트론의 동작 방법과 동일</p>
<h3 id="순방향--출력--계산">순방향  출력  계산</h3>
<blockquote>
<p>current = input
for each layer in network 
for each neuron i in layer
net = get_sum(weights[i], current)  # 입력의 가중합 계산 
output[i] = activation_func(net)    # 각 노드들의 출력 계산
current = output                    # 다음 계층은 현재 계층의 출력을 입력으로 사용</p>
</blockquote>
<h3 id="다층--퍼셉트론의--학습--">다층  퍼셉트론의  학습 -&gt;</h3>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/3f392411-aca5-417e-afa6-f49889ff51a3/image.png" /></p>
<p>• 다층 퍼셉트론에서 순방향 학습이 가능한가?
• 오차 역전파(error back-propagation) 알고리즘
⁻   입력이 주어지면 순방향으로 계산하여 출력을 계산한 후에 실제 출력과 우리가 원하는 출력 간의 
오차를 계산
⁻   이 오차를 역방향으로 전파하면서 오차를 줄이는 방향으로 가중치를 변경</p>
<h3 id="오차--역전파--알고리즘--의사--코드">오차  역전파  알고리즘  의사  코드</h3>
<blockquote>
<p>신경망의 가중치를 작은 난수로 초기화한다.
do 각 훈련 샘플 sample에 대하여 다음을 반복한다. 
    actual = calculate_network(sample)  // 순방향 패스 
    target = desired_output(sample)
    각 출력 노드에서 오차(target - actual)을 계산한다.
    은닉층에서 출력층으로의 가중치 변경값을 계산한다. // 역방향 패스 
    입력층에서 은닉층으로의 가중치 변경값을 계산한다. // 역방향 패스 
    전체 가중치를 업데이트한다.
until 모든 샘플이 올바르게 분류될 때까지 -&gt; 끝나지 않을 수도..</p>
</blockquote>
<h3 id="손실--함수---학습을-못한걸-표현해준다-작을수록-좋다">손실  함수 -&gt; 학습을 못한걸 표현해준다. (작을수록 좋다.)</h3>
<h4 id="-지금-당신은-얼마나-행복한가요">• 지금 당신은 얼마나 행복한가요?</h4>
<p>⁻   내 행복지수는 10.23입니다.
⁻   행복함의 정도를 행복지수라는 하나의 지표로 표현</p>
<h4 id="-손실-함수loss-function">• 손실 함수(loss function)</h4>
<p>⁻   신경망 학습에서는 현재의 상태(성능)를 ‘하나의 지표’, 손실 함수로 표현
⁻   신경망 학습: 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것 
⁻   손실 함수는 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하는지를 표현 
⁻   손실 함수는 일반적으로 평균 제곱 오차와 교차 엔트로피 오차 사용</p>
<h4 id="-정확도-대신-손실-함수의-값을-지표로-사용하는-이유는">• ‘정확도’ 대신 ‘손실 함수의 값’을 지표로 사용하는 이유는?</h4>
<p>⁻   정확도는 가중치의 미세한 변화에는 거의 반응을 보이지 않음
⁻ 손실 함수는 매개변수의 값이 조금 변하면 그에 반응하여 손실 함수의 값도 연속적으로 변화함
• 손실 함수 값의 변화에 따라 적절하게 매개변수를 갱신할 수 있음</p>
<h4 id="경사--하강법-손실함수---가중치-손실함수">경사  하강법 손실함수 -&gt; 가중치, 손실함수</h4>
<p><img alt="" src="https://velog.velcdn.com/images/mi_nini/post/836ee1fc-4204-4428-9d6c-0e19b07aa047/image.png" />
• 정확한 기울기를 계산하려면 전체 데이터에 대해서 연산을 해야 해서 효율성에 
대한 문제 발생</p>
<p>⁻   전체 데이터가 아니라 하나의 샘플에서 비용함수의 경사를 계산하여 경사하강법을 사용해도 대체 
로 괜찮은 결과를 얻을 수 있다는 것이 경험적으로 입증
= 확률적 경사하강법(stochastic gradient descent, SGD) </p>
<p>⁻   실제로는 하나의 샘플만을 사용하지는 않고, 전체 데이터 중 10개에서 1,000개 정도의 샘플이 포 
함된 배치를 사용
= 미니 배치 경사하강법(mini-batch gradient descent)</p>
<p>w= w(가중치)− α(학습률) = 𝜕𝐿/𝜕𝑤 (기울기)</p>